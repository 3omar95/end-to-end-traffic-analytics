{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8987344,"sourceType":"datasetVersion","datasetId":5412716},{"sourceId":9045576,"sourceType":"datasetVersion","datasetId":5453573}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing the Object Detection API","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models.git","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:29:43.591240Z","iopub.execute_input":"2024-07-27T13:29:43.591843Z","iopub.status.idle":"2024-07-27T13:30:10.049357Z","shell.execute_reply.started":"2024-07-27T13:29:43.591808Z","shell.execute_reply":"2024-07-27T13:30:10.048456Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'models'...\nremote: Enumerating objects: 97517, done.\u001b[K\nremote: Counting objects: 100% (31/31), done.\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 97517 (delta 13), reused 27 (delta 11), pack-reused 97486\u001b[K\nReceiving objects: 100% (97517/97517), 613.52 MiB | 28.98 MiB/s, done.\nResolving deltas: 100% (71000/71000), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# %mkdir protoc\n# %cd protoc\n# !wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip -q\n# !unzip -o protobuf.zip\n# !rm protobuf.zip","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:30:10.051685Z","iopub.execute_input":"2024-07-27T13:30:10.052068Z","iopub.status.idle":"2024-07-27T13:30:10.056564Z","shell.execute_reply.started":"2024-07-27T13:30:10.052031Z","shell.execute_reply":"2024-07-27T13:30:10.055633Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%cd models/research\n!protoc object_detection/protos/*.proto --python_out=.","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:30:10.057925Z","iopub.execute_input":"2024-07-27T13:30:10.058223Z","iopub.status.idle":"2024-07-27T13:30:11.130400Z","shell.execute_reply.started":"2024-07-27T13:30:10.058194Z","shell.execute_reply":"2024-07-27T13:30:11.129256Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/models/research\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!cp object_detection/packages/tf2/setup.py .\n!pip install . ","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:30:11.133685Z","iopub.execute_input":"2024-07-27T13:30:11.134498Z","iopub.status.idle":"2024-07-27T13:32:00.904542Z","shell.execute_reply.started":"2024-07-27T13:30:11.134463Z","shell.execute_reply":"2024-07-27T13:32:00.903596Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/working/models/research\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting avro-python3 (from object_detection==0.1)\n  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: apache-beam in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (2.46.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (9.5.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (5.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (3.7.5)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (3.0.8)\nCollecting contextlib2 (from object_detection==0.1)\n  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\nCollecting tf-slim (from object_detection==0.1)\n  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (1.16.0)\nCollecting pycocotools (from object_detection==0.1)\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting lvis (from object_detection==0.1)\n  Downloading lvis-0.5.3-py3-none-any.whl.metadata (856 bytes)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (1.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (2.2.2)\nCollecting tf-models-official>=2.5.1 (from object_detection==0.1)\n  Downloading tf_models_official-2.17.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: tensorflow_io in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (0.35.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from object_detection==0.1) (3.4.1)\nCollecting pyparsing==2.4.7 (from object_detection==0.1)\n  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\nCollecting sacrebleu<=2.2.0 (from object_detection==0.1)\n  Downloading sacrebleu-2.2.0-py3-none-any.whl.metadata (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu<=2.2.0->object_detection==0.1)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu<=2.2.0->object_detection==0.1) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu<=2.2.0->object_detection==0.1) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu<=2.2.0->object_detection==0.1) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu<=2.2.0->object_detection==0.1) (0.4.6)\nCollecting gin-config (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (2.136.0)\nCollecting immutabledict (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (1.6.14)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.1.3)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.10.0.84)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (5.9.3)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (9.0.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (6.0.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\nCollecting seqeval (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.9.4)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official>=2.5.1->object_detection==0.1) (0.16.1)\nCollecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\nCollecting tensorflow-text~=2.17.0 (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting tensorflow~=2.17.0 (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting tf-keras>=2.16.0 (from tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->object_detection==0.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->object_detection==0.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->object_detection==0.1) (2023.4)\nRequirement already satisfied: absl-py>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from tf-slim->object_detection==0.1) (1.4.0)\nRequirement already satisfied: protobuf<4,>3.12.2 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (3.20.3)\nRequirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (1.7)\nRequirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (3.9.10)\nCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object_detection==0.1)\n  Downloading dill-0.3.1.1.tar.gz (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (2.2.1)\nRequirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (1.9.3)\nRequirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (0.19)\nRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (1.60.0)\nRequirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (2.7.3)\nRequirement already satisfied: httplib2<0.22.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (0.21.0)\nCollecting numpy>=1.17 (from sacrebleu<=2.2.0->object_detection==0.1)\n  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (0.6.1)\nRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (3.13.0)\nRequirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (1.23.0)\nRequirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (1.4.2)\nRequirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (4.9.0)\nRequirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam->object_detection==0.1) (0.22.0)\nCollecting pyarrow<10.0.0,>=3.0.0 (from apache-beam->object_detection==0.1)\n  Downloading pyarrow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (0.12.1)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (0.2.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras->object_detection==0.1) (21.3)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lvis->object_detection==0.1) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from lvis->object_detection==0.1) (1.4.5)\nRequirement already satisfied: opencv-python>=4.1.0.25 in /opt/conda/lib/python3.10/site-packages (from lvis->object_detection==0.1) (4.10.0.84)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->object_detection==0.1) (1.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->object_detection==0.1) (4.47.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem==0.35.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_io->object_detection==0.1) (0.35.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (2.26.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (3.0.1)\nRequirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object_detection==0.1) (0.6.2)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (2024.7.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (4.66.4)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (6.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.6)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (1.6.3)\nCollecting flatbuffers>=24.3.25 (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (16.0.6)\nCollecting ml-dtypes (from keras->object_detection==0.1)\n  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.3.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (69.0.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (2.4.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (1.14.1)\nCollecting tensorboard<2.18,>=2.17 (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1)\n  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object_detection==0.1) (0.1.8)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (4.9)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->object_detection==0.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->object_detection==0.1) (2.17.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official>=2.5.1->object_detection==0.1) (1.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (8.1.7)\nRequirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (1.6.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.10.2)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.42.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (2024.5.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (3.17.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (1.62.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (4.2.4)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->object_detection==0.1) (0.1.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object_detection==0.1) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object_detection==0.1) (3.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.0.3)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (2.1.3)\nDownloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tf_models_official-2.17.0-py2.py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\nDownloading lvis-0.5.3-py3-none-any.whl (14 kB)\nDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nDownloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: object_detection, avro-python3, dill, seqeval\n  Building wheel for object_detection (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for object_detection: filename=object_detection-0.1-py3-none-any.whl size=21880009 sha256=a585f9c0e8fef3e99bd43dfc20e58b6fa658d4555d22b124c3a4c8695d54acfa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-tosz9nt_/wheels/e6/5c/1f/32444df4025257dccdc9eafab2d06b65752494ee9ca01a388c\n  Building wheel for avro-python3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43992 sha256=446a978ac1c41582d6ea8c43f188fb3c76a8f2992fbb159acc6c567122199241\n  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n  Building wheel for dill (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=ade2f2cdfabfb8e89079827327e14c4550a027e062d340dc2a2563d4d11d5e6c\n  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=c661baeecd0834ee0516c244ccc8204a3f64a18643e08341643178b4d5814e45\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built object_detection avro-python3 dill seqeval\nInstalling collected packages: gin-config, flatbuffers, tf-slim, pyparsing, portalocker, numpy, immutabledict, dill, contextlib2, avro-python3, tensorflow-model-optimization, tensorboard, sacrebleu, pyarrow, ml-dtypes, tensorflow, seqeval, pycocotools, lvis, tf-keras, tensorflow-text, tf-models-official, object_detection\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.1.1\n    Uninstalling pyparsing-3.1.1:\n      Successfully uninstalled pyparsing-3.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 16.1.0\n    Uninstalling pyarrow-16.1.0:\n      Successfully uninstalled pyarrow-16.1.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.15.1\n    Uninstalling tf_keras-2.15.1:\n      Successfully uninstalled tf_keras-2.15.1\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.15.0\n    Uninstalling tensorflow-text-2.15.0:\n      Successfully uninstalled tensorflow-text-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nucx-py 0.38.0 requires libucx<1.16,>=1.15.0, which is not installed.\nucxx 0.38.0 requires libucx>=1.15.0, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.6.1 requires pyarrow<16.2.0a0,>=16.1.0, but you have pyarrow 9.0.0 which is incompatible.\ndatasets 2.20.0 requires pyarrow>=15.0.0, but you have pyarrow 9.0.0 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nmultiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\npytoolconfig 1.3.1 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nrmm 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.17.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\nxarray 2024.6.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed avro-python3-1.10.2 contextlib2-21.6.0 dill-0.3.1.1 flatbuffers-24.3.25 gin-config-0.5.0 immutabledict-4.2.0 lvis-0.5.3 ml-dtypes-0.4.0 numpy-1.24.4 object_detection-0.1 portalocker-2.10.1 pyarrow-9.0.0 pycocotools-2.0.8 pyparsing-2.4.7 sacrebleu-2.2.0 seqeval-1.2.2 tensorboard-2.17.0 tensorflow-2.17.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.17.0 tf-keras-2.17.0 tf-models-official-2.17.0 tf-slim-1.1.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install tensorflow==2.15\n!pip install tf-slim\n!pip install pycocotools\n!pip install lvis\n!pip install tf-models-official==2.15\n!pip install tf-keras==2.15","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:32:00.906336Z","iopub.execute_input":"2024-07-27T13:32:00.906751Z","iopub.status.idle":"2024-07-27T13:33:53.992593Z","shell.execute_reply.started":"2024-07-27T13:32:00.906716Z","shell.execute_reply":"2024-07-27T13:33:53.991409Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.15\n  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (16.0.6)\nCollecting ml-dtypes~=0.2.0 (from tensorflow==2.15)\n  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.24.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.60.0)\nCollecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15) (2.4.7)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\nDownloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ml-dtypes, keras, tensorboard, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.4.0\n    Uninstalling ml-dtypes-0.4.0:\n      Successfully uninstalled ml-dtypes-0.4.0\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.0\n    Uninstalling tensorboard-2.17.0:\n      Successfully uninstalled tensorboard-2.17.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.0\n    Uninstalling tensorflow-2.17.0:\n      Successfully uninstalled tensorflow-2.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.15.0 which is incompatible.\ntensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\ntf-models-official 2.17.0 requires tensorflow~=2.17.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0\nRequirement already satisfied: tf-slim in /opt/conda/lib/python3.10/site-packages (1.1.0)\nRequirement already satisfied: absl-py>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from tf-slim) (1.4.0)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.8)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.24.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nRequirement already satisfied: lvis in /opt/conda/lib/python3.10/site-packages (0.5.3)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lvis) (0.12.1)\nRequirement already satisfied: Cython>=0.29.12 in /opt/conda/lib/python3.10/site-packages (from lvis) (3.0.8)\nRequirement already satisfied: kiwisolver>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from lvis) (1.4.5)\nRequirement already satisfied: matplotlib>=3.1.1 in /opt/conda/lib/python3.10/site-packages (from lvis) (3.7.5)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from lvis) (1.24.4)\nRequirement already satisfied: opencv-python>=4.1.0.25 in /opt/conda/lib/python3.10/site-packages (from lvis) (4.10.0.84)\nRequirement already satisfied: pyparsing>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from lvis) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from lvis) (2.9.0.post0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from lvis) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->lvis) (1.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->lvis) (4.47.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->lvis) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->lvis) (9.5.0)\nCollecting tf-models-official==2.15\n  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (3.0.8)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (9.5.0)\nRequirement already satisfied: gin-config in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.5.0)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.136.0)\nRequirement already satisfied: immutabledict in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.2.0)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.6.14)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (3.7.5)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.24.4)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.1.3)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.10.0.84)\nRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.2.2)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (5.9.3)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (9.0.0)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.0.8)\nRequirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (6.0.1)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.2.0)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.11.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.2.0)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.16.0)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.9.4)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.16.1)\nRequirement already satisfied: tensorflow-model-optimization>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.8.0)\nCollecting tensorflow-text~=2.15.0 (from tf-models-official==2.15)\n  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nRequirement already satisfied: tensorflow~=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.15.0)\nRequirement already satisfied: tf-slim>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.1.0)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (0.21.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (2.26.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (3.0.1)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2024.7.4)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (4.66.4)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (6.1.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15) (2023.4)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (69.0.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.2)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.0)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.15) (2.17.0)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.15) (0.1.8)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (2.4.7)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (4.9)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (2.10.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (5.2.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official==2.15) (1.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (8.1.7)\nRequirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (1.6.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.10.2)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-models-official==2.15) (0.42.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (2024.5.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (3.17.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.15) (1.62.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.15) (4.2.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15) (3.6)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15) (3.2.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.0.3)\nINFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\nCollecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.15)\n  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.15) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.15) (1.3)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (2.1.3)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.2.2)\nDownloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tf-keras, tensorflow-text, tf-models-official\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.17.0\n    Uninstalling tf_keras-2.17.0:\n      Successfully uninstalled tf_keras-2.17.0\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.17.0\n    Uninstalling tensorflow-text-2.17.0:\n      Successfully uninstalled tensorflow-text-2.17.0\n  Attempting uninstall: tf-models-official\n    Found existing installation: tf-models-official 2.17.0\n    Uninstalling tf-models-official-2.17.0:\n      Successfully uninstalled tf-models-official-2.17.0\nSuccessfully installed tensorflow-text-2.15.0 tf-keras-2.15.1 tf-models-official-2.15.0\nCollecting tf-keras==2.15\n  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\nDownloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tf-keras\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.15.1\n    Uninstalling tf_keras-2.15.1:\n      Successfully uninstalled tf_keras-2.15.1\nSuccessfully installed tf-keras-2.15.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# !python object_detection/builders/model_builder_tf2_test.py","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:53.994235Z","iopub.execute_input":"2024-07-27T13:33:53.994595Z","iopub.status.idle":"2024-07-27T13:33:53.998931Z","shell.execute_reply.started":"2024-07-27T13:33:53.994564Z","shell.execute_reply":"2024-07-27T13:33:53.998101Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# file = '''# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n# #\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# # ==============================================================================\n\n# \"\"\"A wrapper around the Keras Resnet V1 models for object detection.\"\"\"\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\n# import tensorflow.compat.v1 as tf\n\n# from object_detection.core import freezable_batch_norm\n# from object_detection.models.keras_models import model_utils\n\n# try:\n#   from keras.applications import resnet  # pylint: disable=g-import-not-at-top\n# except ImportError:\n#   from tf_keras.applications import resnet  # pylint: disable=g-import-not-at-top\n\n\n# def _fixed_padding(inputs, kernel_size, rate=1):  # pylint: disable=invalid-name\n#   \"\"\"Pads the input along the spatial dimensions independently of input size.\n\n#   Pads the input such that if it was used in a convolution with 'VALID' padding,\n#   the output would have the same dimensions as if the unpadded input was used\n#   in a convolution with 'SAME' padding.\n\n#   Args:\n#     inputs: A tensor of size [batch, height_in, width_in, channels].\n#     kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n#     rate: An integer, rate for atrous convolution.\n\n#   Returns:\n#     output: A tensor of size [batch, height_out, width_out, channels] with the\n#       input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n#   \"\"\"\n#   kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n#   pad_total = kernel_size_effective - 1\n#   pad_beg = pad_total // 2\n#   pad_end = pad_total - pad_beg\n#   padded_inputs = tf.pad(\n#       inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n#   return padded_inputs\n\n\n# class _LayersOverride(object):\n#   \"\"\"Alternative Keras layers interface for the Keras Resnet V1.\"\"\"\n\n#   def __init__(self,\n#                batchnorm_training,\n#                batchnorm_scale=True,\n#                default_batchnorm_momentum=0.997,\n#                default_batchnorm_epsilon=1e-5,\n#                weight_decay=0.0001,\n#                conv_hyperparams=None,\n#                min_depth=8,\n#                depth_multiplier=1):\n#     \"\"\"Alternative tf.keras.layers interface, for use by the Keras Resnet V1.\n\n#     The class is used by the Keras applications kwargs injection API to\n#     modify the Resnet V1 Keras application with changes required by\n#     the Object Detection API.\n\n#     Args:\n#       batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n#         when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n#       batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale\n#         the activations in the batch normalization layer.\n#       default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,\n#         batch norm layers will be constructed using this value as the momentum.\n#       default_batchnorm_epsilon: Float. When 'conv_hyperparams' is None,\n#         batch norm layers will be constructed using this value as the epsilon.\n#       weight_decay: The weight decay to use for regularizing the model.\n#       conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n#         containing hyperparameters for convolution ops. Optionally set to `None`\n#         to use default resnet_v1 layer builders.\n#       min_depth: Minimum number of filters in the convolutional layers.\n#       depth_multiplier: The depth multiplier to modify the number of filters\n#         in the convolutional layers.\n#     \"\"\"\n#     self._batchnorm_training = batchnorm_training\n#     self._batchnorm_scale = batchnorm_scale\n#     self._default_batchnorm_momentum = default_batchnorm_momentum\n#     self._default_batchnorm_epsilon = default_batchnorm_epsilon\n#     self._conv_hyperparams = conv_hyperparams\n#     self._min_depth = min_depth\n#     self._depth_multiplier = depth_multiplier\n#     self.regularizer = tf.keras.regularizers.l2(weight_decay)\n#     self.initializer = tf.variance_scaling_initializer()\n\n#   def _FixedPaddingLayer(self, kernel_size, rate=1):  # pylint: disable=invalid-name\n#     return tf.keras.layers.Lambda(\n#         lambda x: _fixed_padding(x, kernel_size, rate))\n\n#   def Conv2D(self, filters, kernel_size, **kwargs):  # pylint: disable=invalid-name\n#     \"\"\"Builds a Conv2D layer according to the current Object Detection config.\n\n#     Overrides the Keras Resnet application's convolutions with ones that\n#     follow the spec specified by the Object Detection hyperparameters.\n\n#     Args:\n#       filters: The number of filters to use for the convolution.\n#       kernel_size: The kernel size to specify the height and width of the 2D\n#         convolution window.\n#       **kwargs: Keyword args specified by the Keras application for\n#         constructing the convolution.\n\n#     Returns:\n#       A one-arg callable that will either directly apply a Keras Conv2D layer to\n#       the input argument, or that will first pad the input then apply a Conv2D\n#       layer.\n#     \"\"\"\n#     # Apply the minimum depth to the convolution layers.\n#     filters = max(int(filters * self._depth_multiplier), self._min_depth)\n\n#     if self._conv_hyperparams:\n#       kwargs = self._conv_hyperparams.params(**kwargs)\n#     else:\n#       kwargs['kernel_regularizer'] = self.regularizer\n#       kwargs['kernel_initializer'] = self.initializer\n\n#     # Set use_bias as false to keep it consistent with Slim Resnet model.\n#     kwargs['use_bias'] = False\n\n#     kwargs['padding'] = 'same'\n#     stride = kwargs.get('strides')\n#     if stride and kernel_size and stride > 1 and kernel_size > 1:\n#       kwargs['padding'] = 'valid'\n#       def padded_conv(features):  # pylint: disable=invalid-name\n#         padded_features = self._FixedPaddingLayer(kernel_size)(features)\n#         return tf.keras.layers.Conv2D(\n#             filters, kernel_size, **kwargs)(padded_features)\n#       return padded_conv\n#     else:\n#       return tf.keras.layers.Conv2D(filters, kernel_size, **kwargs)\n\n#   def Activation(self, *args, **kwargs):  # pylint: disable=unused-argument,invalid-name\n#     \"\"\"Builds an activation layer.\n\n#     Overrides the Keras application Activation layer specified by the\n#     Object Detection configuration.\n\n#     Args:\n#       *args: Ignored,\n#         required to match the `tf.keras.layers.Activation` interface.\n#       **kwargs: Only the name is used,\n#         required to match `tf.keras.layers.Activation` interface.\n\n#     Returns:\n#       An activation layer specified by the Object Detection hyperparameter\n#       configurations.\n#     \"\"\"\n#     name = kwargs.get('name')\n#     if self._conv_hyperparams:\n#       return self._conv_hyperparams.build_activation_layer(name=name)\n#     else:\n#       return tf.keras.layers.Lambda(tf.nn.relu, name=name)\n\n#   def BatchNormalization(self, **kwargs):  # pylint: disable=invalid-name\n#     \"\"\"Builds a normalization layer.\n\n#     Overrides the Keras application batch norm with the norm specified by the\n#     Object Detection configuration.\n\n#     Args:\n#       **kwargs: Only the name is used, all other params ignored.\n#         Required for matching `layers.BatchNormalization` calls in the Keras\n#         application.\n\n#     Returns:\n#       A normalization layer specified by the Object Detection hyperparameter\n#       configurations.\n#     \"\"\"\n#     name = kwargs.get('name')\n#     if self._conv_hyperparams:\n#       return self._conv_hyperparams.build_batch_norm(\n#           training=self._batchnorm_training,\n#           name=name)\n#     else:\n#       kwargs['scale'] = self._batchnorm_scale\n#       kwargs['epsilon'] = self._default_batchnorm_epsilon\n#       return freezable_batch_norm.FreezableBatchNorm(\n#           training=self._batchnorm_training,\n#           momentum=self._default_batchnorm_momentum,\n#           **kwargs)\n\n#   def Input(self, shape):  # pylint: disable=invalid-name\n#     \"\"\"Builds an Input layer.\n\n#     Overrides the Keras application Input layer with one that uses a\n#     tf.placeholder_with_default instead of a tf.placeholder. This is necessary\n#     to ensure the application works when run on a TPU.\n\n#     Args:\n#       shape: A tuple of integers representing the shape of the input, which\n#         includes both spatial share and channels, but not the batch size.\n#         Elements of this tuple can be None; 'None' elements represent dimensions\n#         where the shape is not known.\n\n#     Returns:\n#       An input layer for the specified shape that internally uses a\n#       placeholder_with_default.\n#     \"\"\"\n#     default_size = 224\n#     default_batch_size = 1\n#     shape = list(shape)\n#     default_shape = [default_size if dim is None else dim for dim in shape]\n\n#     input_tensor = tf.constant(0.0, shape=[default_batch_size] + default_shape)\n\n#     placeholder_with_default = tf.placeholder_with_default(\n#         input=input_tensor, shape=[None] + shape)\n#     return model_utils.input_layer(shape, placeholder_with_default)\n\n#   def MaxPooling2D(self, pool_size, **kwargs):  # pylint: disable=invalid-name\n#     \"\"\"Builds a MaxPooling2D layer with default padding as 'SAME'.\n\n#     This is specified by the default resnet arg_scope in slim.\n\n#     Args:\n#       pool_size: The pool size specified by the Keras application.\n#       **kwargs: Ignored, required to match the Keras applications usage.\n\n#     Returns:\n#       A MaxPooling2D layer with default padding as 'SAME'.\n#     \"\"\"\n#     kwargs['padding'] = 'same'\n#     return tf.keras.layers.MaxPooling2D(pool_size, **kwargs)\n\n#   # Add alias as Keras also has it.\n#   MaxPool2D = MaxPooling2D  # pylint: disable=invalid-name\n\n#   def ZeroPadding2D(self, padding, **kwargs):  # pylint: disable=unused-argument,invalid-name\n#     \"\"\"Replaces explicit padding in the Keras application with a no-op.\n\n#     Args:\n#       padding: The padding values for image height and width.\n#       **kwargs: Ignored, required to match the Keras applications usage.\n\n#     Returns:\n#       A no-op identity lambda.\n#     \"\"\"\n#     return lambda x: x\n\n#   # Forward all non-overridden methods to the keras layers\n#   def __getattr__(self, item):\n#     return getattr(tf.keras.layers, item)\n\n\n# # pylint: disable=invalid-name\n# def resnet_v1_50(batchnorm_training,\n#                  batchnorm_scale=True,\n#                  default_batchnorm_momentum=0.997,\n#                  default_batchnorm_epsilon=1e-5,\n#                  weight_decay=0.0001,\n#                  conv_hyperparams=None,\n#                  min_depth=8,\n#                  depth_multiplier=1,\n#                  **kwargs):\n#   \"\"\"Instantiates the Resnet50 architecture, modified for object detection.\n\n#   Args:\n#     batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n#       when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n#     batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale\n#       the activations in the batch normalization layer.\n#     default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the momentum.\n#     default_batchnorm_epsilon: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the epsilon.\n#     weight_decay: The weight decay to use for regularizing the model.\n#     conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n#       containing hyperparameters for convolution ops. Optionally set to `None`\n#       to use default resnet_v1 layer builders.\n#     min_depth: Minimum number of filters in the convolutional layers.\n#     depth_multiplier: The depth multiplier to modify the number of filters\n#       in the convolutional layers.\n#     **kwargs: Keyword arguments forwarded directly to the\n#       `tf.keras.applications.Mobilenet` method that constructs the Keras\n#       model.\n\n#   Returns:\n#     A Keras ResnetV1-50 model instance.\n#   \"\"\"\n#   layers_override = _LayersOverride(\n#       batchnorm_training,\n#       batchnorm_scale=batchnorm_scale,\n#       default_batchnorm_momentum=default_batchnorm_momentum,\n#       default_batchnorm_epsilon=default_batchnorm_epsilon,\n#       conv_hyperparams=conv_hyperparams,\n#       weight_decay=weight_decay,\n#       min_depth=min_depth,\n#       depth_multiplier=depth_multiplier)\n#   return tf.keras.applications.resnet.ResNet50(\n#       layers=layers_override, **kwargs)\n\n\n# def resnet_v1_101(batchnorm_training,\n#                   batchnorm_scale=True,\n#                   default_batchnorm_momentum=0.997,\n#                   default_batchnorm_epsilon=1e-5,\n#                   weight_decay=0.0001,\n#                   conv_hyperparams=None,\n#                   min_depth=8,\n#                   depth_multiplier=1,\n#                   **kwargs):\n#   \"\"\"Instantiates the Resnet50 architecture, modified for object detection.\n\n#   Args:\n#     batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n#       when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n#     batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale\n#       the activations in the batch normalization layer.\n#     default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the momentum.\n#     default_batchnorm_epsilon: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the epsilon.\n#     weight_decay: The weight decay to use for regularizing the model.\n#     conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n#       containing hyperparameters for convolution ops. Optionally set to `None`\n#       to use default resnet_v1 layer builders.\n#     min_depth: Minimum number of filters in the convolutional layers.\n#     depth_multiplier: The depth multiplier to modify the number of filters\n#       in the convolutional layers.\n#     **kwargs: Keyword arguments forwarded directly to the\n#       `tf.keras.applications.Mobilenet` method that constructs the Keras\n#       model.\n\n#   Returns:\n#     A Keras ResnetV1-101 model instance.\n#   \"\"\"\n#   layers_override = _LayersOverride(\n#       batchnorm_training,\n#       batchnorm_scale=batchnorm_scale,\n#       default_batchnorm_momentum=default_batchnorm_momentum,\n#       default_batchnorm_epsilon=default_batchnorm_epsilon,\n#       conv_hyperparams=conv_hyperparams,\n#       weight_decay=weight_decay,\n#       min_depth=min_depth,\n#       depth_multiplier=depth_multiplier)\n#   return tf.keras.applications.resnet.ResNet101(\n#       layers=layers_override, **kwargs)\n\n\n# def resnet_v1_152(batchnorm_training,\n#                   batchnorm_scale=True,\n#                   default_batchnorm_momentum=0.997,\n#                   default_batchnorm_epsilon=1e-5,\n#                   weight_decay=0.0001,\n#                   conv_hyperparams=None,\n#                   min_depth=8,\n#                   depth_multiplier=1,\n#                   **kwargs):\n#   \"\"\"Instantiates the Resnet50 architecture, modified for object detection.\n\n#   Args:\n#     batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n#       when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n#     batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale\n#       the activations in the batch normalization layer.\n#     default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the momentum.\n#     default_batchnorm_epsilon: Float. When 'conv_hyperparams' is None,\n#       batch norm layers will be constructed using this value as the epsilon.\n#     weight_decay: The weight decay to use for regularizing the model.\n#     conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n#       containing hyperparameters for convolution ops. Optionally set to `None`\n#       to use default resnet_v1 layer builders.\n#     min_depth: Minimum number of filters in the convolutional layers.\n#     depth_multiplier: The depth multiplier to modify the number of filters\n#       in the convolutional layers.\n#     **kwargs: Keyword arguments forwarded directly to the\n#       `tf.keras.applications.Mobilenet` method that constructs the Keras\n#       model.\n\n#   Returns:\n#     A Keras ResnetV1-152 model instance.\n#   \"\"\"\n#   layers_override = _LayersOverride(\n#       batchnorm_training,\n#       batchnorm_scale=batchnorm_scale,\n#       default_batchnorm_momentum=default_batchnorm_momentum,\n#       default_batchnorm_epsilon=default_batchnorm_epsilon,\n#       conv_hyperparams=conv_hyperparams,\n#       weight_decay=weight_decay,\n#       min_depth=min_depth,\n#       depth_multiplier=depth_multiplier)\n#   return tf.keras.applications.resnet.ResNet152(\n#       layers=layers_override, **kwargs)\n# # pylint: enable=invalid-name\n\n\n# # The following codes are based on the existing keras ResNet model pattern:\n# # google3/third_party/py/keras/applications/resnet.py\n# def block_basic(x,\n#                 filters,\n#                 kernel_size=3,\n#                 stride=1,\n#                 conv_shortcut=False,\n#                 name=None):\n#   \"\"\"A residual block for ResNet18/34.\n\n#   Args:\n#       x: input tensor.\n#       filters: integer, filters of the bottleneck layer.\n#       kernel_size: default 3, kernel size of the bottleneck layer.\n#       stride: default 1, stride of the first layer.\n#       conv_shortcut: default False, use convolution shortcut if True, otherwise\n#         identity shortcut.\n#       name: string, block label.\n\n#   Returns:\n#     Output tensor for the residual block.\n#   \"\"\"\n#   layers = tf.keras.layers\n#   bn_axis = 3 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n\n#   preact = layers.BatchNormalization(\n#       axis=bn_axis, epsilon=1.001e-5, name=name + '_preact_bn')(\n#           x)\n#   preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\n\n#   if conv_shortcut:\n#     shortcut = layers.Conv2D(\n#         filters, 1, strides=1, name=name + '_0_conv')(\n#             preact)\n#   else:\n#     shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\n\n#   x = layers.ZeroPadding2D(\n#       padding=((1, 1), (1, 1)), name=name + '_1_pad')(\n#           preact)\n#   x = layers.Conv2D(\n#       filters, kernel_size, strides=1, use_bias=False, name=name + '_1_conv')(\n#           x)\n#   x = layers.BatchNormalization(\n#       axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(\n#           x)\n#   x = layers.Activation('relu', name=name + '_1_relu')(x)\n\n#   x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n#   x = layers.Conv2D(\n#       filters,\n#       kernel_size,\n#       strides=stride,\n#       use_bias=False,\n#       name=name + '_2_conv')(\n#           x)\n#   x = layers.BatchNormalization(\n#       axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(\n#           x)\n#   x = layers.Activation('relu', name=name + '_2_relu')(x)\n#   x = layers.Add(name=name + '_out')([shortcut, x])\n#   return x\n\n\n# def stack_basic(x, filters, blocks, stride1=2, name=None):\n#   \"\"\"A set of stacked residual blocks for ResNet18/34.\n\n#   Args:\n#       x: input tensor.\n#       filters: integer, filters of the bottleneck layer in a block.\n#       blocks: integer, blocks in the stacked blocks.\n#       stride1: default 2, stride of the first layer in the first block.\n#       name: string, stack label.\n\n#   Returns:\n#       Output tensor for the stacked blocks.\n#   \"\"\"\n#   x = block_basic(x, filters, conv_shortcut=True, name=name + '_block1')\n#   for i in range(2, blocks):\n#     x = block_basic(x, filters, name=name + '_block' + str(i))\n#   x = block_basic(\n#       x, filters, stride=stride1, name=name + '_block' + str(blocks))\n#   return x\n\n\n# def resnet_v1_18(include_top=True,\n#                  weights='imagenet',\n#                  input_tensor=None,\n#                  input_shape=None,\n#                  pooling=None,\n#                  classes=1000,\n#                  classifier_activation='softmax'):\n#   \"\"\"Instantiates the ResNet18 architecture.\"\"\"\n\n#   def stack_fn(x):\n#     x = stack_basic(x, 64, 2, stride1=1, name='conv2')\n#     x = stack_basic(x, 128, 2, name='conv3')\n#     x = stack_basic(x, 256, 2, name='conv4')\n#     return stack_basic(x, 512, 2, name='conv5')\n\n#   return resnet.ResNet(\n#       stack_fn,\n#       True,\n#       True,\n#       'resnet18',\n#       include_top,\n#       weights,\n#       input_tensor,\n#       input_shape,\n#       pooling,\n#       classes,\n#       classifier_activation=classifier_activation)\n\n\n# def resnet_v1_34(include_top=True,\n#                  weights='imagenet',\n#                  input_tensor=None,\n#                  input_shape=None,\n#                  pooling=None,\n#                  classes=1000,\n#                  classifier_activation='softmax'):\n#   \"\"\"Instantiates the ResNet34 architecture.\"\"\"\n\n#   def stack_fn(x):\n#     x = stack_basic(x, 64, 3, stride1=1, name='conv2')\n#     x = stack_basic(x, 128, 4, name='conv3')\n#     x = stack_basic(x, 256, 6, name='conv4')\n#     return stack_basic(x, 512, 3, name='conv5')\n\n#   return resnet.ResNet(\n#       stack_fn,\n#       True,\n#       True,\n#       'resnet34',\n#       include_top,\n#       weights,\n#       input_tensor,\n#       input_shape,\n#       pooling,\n#       classes,\n#       classifier_activation=classifier_activation)'''\n\n# with open('/opt/conda/lib/python3.10/site-packages/object_detection/models/keras_models/resnet_v1.py', 'w') as f:\n#     f.write(file)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-27T13:33:54.000590Z","iopub.execute_input":"2024-07-27T13:33:54.000879Z","iopub.status.idle":"2024-07-27T13:33:54.028118Z","shell.execute_reply.started":"2024-07-27T13:33:54.000855Z","shell.execute_reply":"2024-07-27T13:33:54.027226Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# file = '''# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n# #\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n\n# \"\"\"AutoAugment and RandAugment policies for enhanced image preprocessing.\n\n# AutoAugment Reference: https://arxiv.org/abs/1805.09501\n# RandAugment Reference: https://arxiv.org/abs/1909.13719\n# \"\"\"\n\n# from __future__ import absolute_import\n# from __future__ import division\n# # from __future__ import google_type_annotations\n# from __future__ import print_function\n\n# import math\n\n# import tensorflow as tf\n# from typing import Any, Dict, List, Optional, Text, Tuple\n\n# from tensorflow.keras.preprocessing import image as image_ops\n\n# # This signifies the max integer that the controller RNN could predict for the\n# # augmentation scheme.\n# _MAX_LEVEL = 10.\n\n\n# def to_4d(image: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Converts an input Tensor to 4 dimensions.\n\n#   4D image => [N, H, W, C] or [N, C, H, W]\n#   3D image => [1, H, W, C] or [1, C, H, W]\n#   2D image => [1, H, W, 1]\n\n#   Args:\n#     image: The 2/3/4D input tensor.\n\n#   Returns:\n#     A 4D image tensor.\n\n#   Raises:\n#     `TypeError` if `image` is not a 2/3/4D tensor.\n\n#   \"\"\"\n#   shape = tf.shape(image)\n#   original_rank = tf.rank(image)\n#   left_pad = tf.cast(tf.less_equal(original_rank, 3), dtype=tf.int32)\n#   right_pad = tf.cast(tf.equal(original_rank, 2), dtype=tf.int32)\n#   new_shape = tf.concat(\n#       [\n#           tf.ones(shape=left_pad, dtype=tf.int32),\n#           shape,\n#           tf.ones(shape=right_pad, dtype=tf.int32),\n#       ],\n#       axis=0,\n#   )\n#   return tf.reshape(image, new_shape)\n\n\n# def from_4d(image: tf.Tensor, ndims: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Converts a 4D image back to `ndims` rank.\"\"\"\n#   shape = tf.shape(image)\n#   begin = tf.cast(tf.less_equal(ndims, 3), dtype=tf.int32)\n#   end = 4 - tf.cast(tf.equal(ndims, 2), dtype=tf.int32)\n#   new_shape = shape[begin:end]\n#   return tf.reshape(image, new_shape)\n\n\n# def _convert_translation_to_transform(translations: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Converts translations to a projective transform.\n\n#   The translation matrix looks like this:\n#     [[1 0 -dx]\n#      [0 1 -dy]\n#      [0 0 1]]\n\n#   Args:\n#     translations: The 2-element list representing [dx, dy], or a matrix of\n#       2-element lists representing [dx dy] to translate for each image. The\n#       shape must be static.\n\n#   Returns:\n#     The transformation matrix of shape (num_images, 8).\n\n#   Raises:\n#     `TypeError` if\n#       - the shape of `translations` is not known or\n#       - the shape of `translations` is not rank 1 or 2.\n\n#   \"\"\"\n#   translations = tf.convert_to_tensor(translations, dtype=tf.float32)\n#   if translations.get_shape().ndims is None:\n#     raise TypeError('translations rank must be statically known')\n#   elif len(translations.get_shape()) == 1:\n#     translations = translations[None]\n#   elif len(translations.get_shape()) != 2:\n#     raise TypeError('translations should have rank 1 or 2.')\n#   num_translations = tf.shape(translations)[0]\n\n#   return tf.concat(\n#       values=[\n#           tf.ones((num_translations, 1), tf.dtypes.float32),\n#           tf.zeros((num_translations, 1), tf.dtypes.float32),\n#           -translations[:, 0, None],\n#           tf.zeros((num_translations, 1), tf.dtypes.float32),\n#           tf.ones((num_translations, 1), tf.dtypes.float32),\n#           -translations[:, 1, None],\n#           tf.zeros((num_translations, 2), tf.dtypes.float32),\n#       ],\n#       axis=1,\n#   )\n\n\n# def _convert_angles_to_transform(angles: tf.Tensor, image_width: tf.Tensor,\n#                                  image_height: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Converts an angle or angles to a projective transform.\n\n#   Args:\n#     angles: A scalar to rotate all images, or a vector to rotate a batch of\n#       images. This must be a scalar.\n#     image_width: The width of the image(s) to be transformed.\n#     image_height: The height of the image(s) to be transformed.\n\n#   Returns:\n#     A tensor of shape (num_images, 8).\n\n#   Raises:\n#     `TypeError` if `angles` is not rank 0 or 1.\n\n#   \"\"\"\n#   angles = tf.convert_to_tensor(angles, dtype=tf.float32)\n#   if len(angles.get_shape()) == 0:  # pylint:disable=g-explicit-length-test\n#     angles = angles[None]\n#   elif len(angles.get_shape()) != 1:\n#     raise TypeError('Angles should have a rank 0 or 1.')\n#   x_offset = ((image_width - 1) -\n#               (tf.math.cos(angles) * (image_width - 1) - tf.math.sin(angles) *\n#                (image_height - 1))) / 2.0\n#   y_offset = ((image_height - 1) -\n#               (tf.math.sin(angles) * (image_width - 1) + tf.math.cos(angles) *\n#                (image_height - 1))) / 2.0\n#   num_angles = tf.shape(angles)[0]\n#   return tf.concat(\n#       values=[\n#           tf.math.cos(angles)[:, None],\n#           -tf.math.sin(angles)[:, None],\n#           x_offset[:, None],\n#           tf.math.sin(angles)[:, None],\n#           tf.math.cos(angles)[:, None],\n#           y_offset[:, None],\n#           tf.zeros((num_angles, 2), tf.dtypes.float32),\n#       ],\n#       axis=1,\n#   )\n\n\n# def transform(image: tf.Tensor, transforms) -> tf.Tensor:\n#   \"\"\"Prepares input data for `image_ops.transform`.\"\"\"\n#   original_ndims = tf.rank(image)\n#   transforms = tf.convert_to_tensor(transforms, dtype=tf.float32)\n#   if transforms.shape.rank == 1:\n#     transforms = transforms[None]\n#   image = to_4d(image)\n#   image = image_ops.transform(\n#       images=image, transforms=transforms, interpolation='nearest')\n#   return from_4d(image, original_ndims)\n\n\n# def translate(image: tf.Tensor, translations) -> tf.Tensor:\n#   \"\"\"Translates image(s) by provided vectors.\n\n#   Args:\n#     image: An image Tensor of type uint8.\n#     translations: A vector or matrix representing [dx dy].\n\n#   Returns:\n#     The translated version of the image.\n\n#   \"\"\"\n#   transforms = _convert_translation_to_transform(translations)\n#   return transform(image, transforms=transforms)\n\n\n# def rotate(image: tf.Tensor, degrees: float) -> tf.Tensor:\n#   \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n\n#   Args:\n#     image: An image Tensor of type uint8.\n#     degrees: Float, a scalar angle in degrees to rotate all images by. If\n#       degrees is positive the image will be rotated clockwise otherwise it will\n#       be rotated counterclockwise.\n\n#   Returns:\n#     The rotated version of image.\n\n#   \"\"\"\n#   # Convert from degrees to radians.\n#   degrees_to_radians = math.pi / 180.0\n#   radians = tf.cast(degrees * degrees_to_radians, tf.float32)\n\n#   original_ndims = tf.rank(image)\n#   image = to_4d(image)\n\n#   image_height = tf.cast(tf.shape(image)[1], tf.float32)\n#   image_width = tf.cast(tf.shape(image)[2], tf.float32)\n#   transforms = _convert_angles_to_transform(\n#       angles=radians, image_width=image_width, image_height=image_height)\n#   # In practice, we should randomize the rotation degrees by flipping\n#   # it negatively half the time, but that's done on 'degrees' outside\n#   # of the function.\n#   image = transform(image, transforms=transforms)\n#   return from_4d(image, original_ndims)\n\n\n# def blend(image1: tf.Tensor, image2: tf.Tensor, factor: float) -> tf.Tensor:\n#   \"\"\"Blend image1 and image2 using 'factor'.\n\n#   Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n#   A value of 1.0 means only image2 is used.  A value between 0.0 and\n#   1.0 means we linearly interpolate the pixel values between the two\n#   images.  A value greater than 1.0 \"extrapolates\" the difference\n#   between the two pixel values, and we clip the results to values\n#   between 0 and 255.\n\n#   Args:\n#     image1: An image Tensor of type uint8.\n#     image2: An image Tensor of type uint8.\n#     factor: A floating point value above 0.0.\n\n#   Returns:\n#     A blended image Tensor of type uint8.\n#   \"\"\"\n#   if factor == 0.0:\n#     return tf.convert_to_tensor(image1)\n#   if factor == 1.0:\n#     return tf.convert_to_tensor(image2)\n\n#   image1 = tf.cast(image1, tf.float32)\n#   image2 = tf.cast(image2, tf.float32)\n\n#   difference = image2 - image1\n#   scaled = factor * difference\n\n#   # Do addition in float.\n#   temp = tf.cast(image1, tf.float32) + scaled\n\n#   # Interpolate\n#   if factor > 0.0 and factor < 1.0:\n#     # Interpolation means we always stay within 0 and 255.\n#     return tf.cast(temp, tf.uint8)\n\n#   # Extrapolate:\n#   #\n#   # We need to clip and then cast.\n#   return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n\n\n# def cutout(image: tf.Tensor, pad_size: int, replace: int = 0) -> tf.Tensor:\n#   \"\"\"Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n\n#   This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n#   a random location within `img`. The pixel values filled in will be of the\n#   value `replace`. The located where the mask will be applied is randomly\n#   chosen uniformly over the whole image.\n\n#   Args:\n#     image: An image Tensor of type uint8.\n#     pad_size: Specifies how big the zero mask that will be generated is that is\n#       applied to the image. The mask will be of size (2*pad_size x 2*pad_size).\n#     replace: What pixel value to fill in the image in the area that has the\n#       cutout mask applied to it.\n\n#   Returns:\n#     An image Tensor that is of type uint8.\n#   \"\"\"\n#   image_height = tf.shape(image)[0]\n#   image_width = tf.shape(image)[1]\n\n#   # Sample the center location in the image where the zero mask will be applied.\n#   cutout_center_height = tf.random.uniform(\n#       shape=[], minval=0, maxval=image_height, dtype=tf.int32)\n\n#   cutout_center_width = tf.random.uniform(\n#       shape=[], minval=0, maxval=image_width, dtype=tf.int32)\n\n#   lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n#   upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n#   left_pad = tf.maximum(0, cutout_center_width - pad_size)\n#   right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n\n#   cutout_shape = [\n#       image_height - (lower_pad + upper_pad),\n#       image_width - (left_pad + right_pad)\n#   ]\n#   padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n#   mask = tf.pad(\n#       tf.zeros(cutout_shape, dtype=image.dtype),\n#       padding_dims,\n#       constant_values=1)\n#   mask = tf.expand_dims(mask, -1)\n#   mask = tf.tile(mask, [1, 1, 3])\n#   image = tf.where(\n#       tf.equal(mask, 0),\n#       tf.ones_like(image, dtype=image.dtype) * replace, image)\n#   return image\n\n\n# def solarize(image: tf.Tensor, threshold: int = 128) -> tf.Tensor:\n#   # For each pixel in the image, select the pixel\n#   # if the value is less than the threshold.\n#   # Otherwise, subtract 255 from the pixel.\n#   return tf.where(image < threshold, image, 255 - image)\n\n\n# def solarize_add(image: tf.Tensor,\n#                  addition: int = 0,\n#                  threshold: int = 128) -> tf.Tensor:\n#   # For each pixel in the image less than threshold\n#   # we add 'addition' amount to it and then clip the\n#   # pixel value to be between 0 and 255. The value\n#   # of 'addition' is between -128 and 128.\n#   added_image = tf.cast(image, tf.int64) + addition\n#   added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n#   return tf.where(image < threshold, added_image, image)\n\n\n# def color(image: tf.Tensor, factor: float) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Color.\"\"\"\n#   degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n#   return blend(degenerate, image, factor)\n\n\n# def contrast(image: tf.Tensor, factor: float) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Contrast.\"\"\"\n#   degenerate = tf.image.rgb_to_grayscale(image)\n#   # Cast before calling tf.histogram.\n#   degenerate = tf.cast(degenerate, tf.int32)\n\n#   # Compute the grayscale histogram, then compute the mean pixel value,\n#   # and create a constant image size of that value.  Use that as the\n#   # blending degenerate target of the original image.\n#   hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n#   mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n#   degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n#   degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n#   degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n#   return blend(degenerate, image, factor)\n\n\n# def brightness(image: tf.Tensor, factor: float) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Brightness.\"\"\"\n#   degenerate = tf.zeros_like(image)\n#   return blend(degenerate, image, factor)\n\n\n# def posterize(image: tf.Tensor, bits: int) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Posterize.\"\"\"\n#   shift = 8 - bits\n#   return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n\n\n# def wrapped_rotate(image: tf.Tensor, degrees: float, replace: int) -> tf.Tensor:\n#   \"\"\"Applies rotation with wrap/unwrap.\"\"\"\n#   image = rotate(wrap(image), degrees=degrees)\n#   return unwrap(image, replace)\n\n\n# def translate_x(image: tf.Tensor, pixels: int, replace: int) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Translate in X dimension.\"\"\"\n#   image = translate(wrap(image), [-pixels, 0])\n#   return unwrap(image, replace)\n\n\n# def translate_y(image: tf.Tensor, pixels: int, replace: int) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n#   image = translate(wrap(image), [0, -pixels])\n#   return unwrap(image, replace)\n\n\n# def shear_x(image: tf.Tensor, level: float, replace: int) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n#   # Shear parallel to x axis is a projective transform\n#   # with a matrix form of:\n#   # [1  level\n#   #  0  1].\n#   image = transform(\n#       image=wrap(image), transforms=[1., level, 0., 0., 1., 0., 0., 0.])\n#   return unwrap(image, replace)\n\n\n# def shear_y(image: tf.Tensor, level: float, replace: int) -> tf.Tensor:\n#   \"\"\"Equivalent of PIL Shearing in Y dimension.\"\"\"\n#   # Shear parallel to y axis is a projective transform\n#   # with a matrix form of:\n#   # [1  0\n#   #  level  1].\n#   image = transform(\n#       image=wrap(image), transforms=[1., 0., 0., level, 1., 0., 0., 0.])\n#   return unwrap(image, replace)\n\n\n# def autocontrast(image: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Implements Autocontrast function from PIL using TF ops.\n\n#   Args:\n#     image: A 3D uint8 tensor.\n\n#   Returns:\n#     The image after it has had autocontrast applied to it and will be of type\n#     uint8.\n#   \"\"\"\n\n#   def scale_channel(image: tf.Tensor) -> tf.Tensor:\n#     \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"\n#     # A possibly cheaper version can be done using cumsum/unique_with_counts\n#     # over the histogram values, rather than iterating over the entire image.\n#     # to compute mins and maxes.\n#     lo = tf.cast(tf.reduce_min(image), tf.float32)\n#     hi = tf.cast(tf.reduce_max(image), tf.float32)\n\n#     # Scale the image, making the lowest value 0 and the highest value 255.\n#     def scale_values(im):\n#       scale = 255.0 / (hi - lo)\n#       offset = -lo * scale\n#       im = tf.cast(im, tf.float32) * scale + offset\n#       im = tf.clip_by_value(im, 0.0, 255.0)\n#       return tf.cast(im, tf.uint8)\n\n#     result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n#     return result\n\n#   # Assumes RGB for now.  Scales each channel independently\n#   # and then stacks the result.\n#   s1 = scale_channel(image[:, :, 0])\n#   s2 = scale_channel(image[:, :, 1])\n#   s3 = scale_channel(image[:, :, 2])\n#   image = tf.stack([s1, s2, s3], 2)\n#   return image\n\n\n# def sharpness(image: tf.Tensor, factor: float) -> tf.Tensor:\n#   \"\"\"Implements Sharpness function from PIL using TF ops.\"\"\"\n#   orig_image = image\n#   image = tf.cast(image, tf.float32)\n#   # Make image 4D for conv operation.\n#   image = tf.expand_dims(image, 0)\n#   # SMOOTH PIL Kernel.\n#   kernel = tf.constant([[1, 1, 1], [1, 5, 1], [1, 1, 1]],\n#                        dtype=tf.float32,\n#                        shape=[3, 3, 1, 1]) / 13.\n#   # Tile across channel dimension.\n#   kernel = tf.tile(kernel, [1, 1, 3, 1])\n#   strides = [1, 1, 1, 1]\n#   degenerate = tf.nn.depthwise_conv2d(\n#       image, kernel, strides, padding='VALID', dilations=[1, 1])\n#   degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n#   degenerate = tf.squeeze(tf.cast(degenerate, tf.uint8), [0])\n\n#   # For the borders of the resulting image, fill in the values of the\n#   # original image.\n#   mask = tf.ones_like(degenerate)\n#   padded_mask = tf.pad(mask, [[1, 1], [1, 1], [0, 0]])\n#   padded_degenerate = tf.pad(degenerate, [[1, 1], [1, 1], [0, 0]])\n#   result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n\n#   # Blend the final result.\n#   return blend(result, orig_image, factor)\n\n\n# def equalize(image: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n\n#   def scale_channel(im, c):\n#     \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n#     im = tf.cast(im[:, :, c], tf.int32)\n#     # Compute the histogram of the image channel.\n#     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n\n#     # For the purposes of computing the step, filter out the nonzeros.\n#     nonzero = tf.where(tf.not_equal(histo, 0))\n#     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n#     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n#     def build_lut(histo, step):\n#       # Compute the cumulative sum, shifting by step // 2\n#       # and then normalization by step.\n#       lut = (tf.cumsum(histo) + (step // 2)) // step\n#       # Shift lut, prepending with 0.\n#       lut = tf.concat([[0], lut[:-1]], 0)\n#       # Clip the counts to be in range.  This is done\n#       # in the C code for image.point.\n#       return tf.clip_by_value(lut, 0, 255)\n\n#     # If step is zero, return the original image.  Otherwise, build\n#     # lut from the full histogram and step and then index from it.\n#     result = tf.cond(\n#         tf.equal(step, 0), lambda: im,\n#         lambda: tf.gather(build_lut(histo, step), im))\n\n#     return tf.cast(result, tf.uint8)\n\n#   # Assumes RGB for now.  Scales each channel independently\n#   # and then stacks the result.\n#   s1 = scale_channel(image, 0)\n#   s2 = scale_channel(image, 1)\n#   s3 = scale_channel(image, 2)\n#   image = tf.stack([s1, s2, s3], 2)\n#   return image\n\n\n# def invert(image: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Inverts the image pixels.\"\"\"\n#   image = tf.convert_to_tensor(image)\n#   return 255 - image\n\n\n# def wrap(image: tf.Tensor) -> tf.Tensor:\n#   \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n#   shape = tf.shape(image)\n#   extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n#   extended = tf.concat([image, extended_channel], axis=2)\n#   return extended\n\n\n# def unwrap(image: tf.Tensor, replace: int) -> tf.Tensor:\n#   \"\"\"Unwraps an image produced by wrap.\n\n#   Where there is a 0 in the last channel for every spatial position,\n#   the rest of the three channels in that spatial dimension are grayed\n#   (set to 128).  Operations like translate and shear on a wrapped\n#   Tensor will leave 0s in empty locations.  Some transformations look\n#   at the intensity of values to do preprocessing, and we want these\n#   empty pixels to assume the 'average' value, rather than pure black.\n\n\n#   Args:\n#     image: A 3D Image Tensor with 4 channels.\n#     replace: A one or three value 1D tensor to fill empty pixels.\n\n#   Returns:\n#     image: A 3D image Tensor with 3 channels.\n#   \"\"\"\n#   image_shape = tf.shape(image)\n#   # Flatten the spatial dimensions.\n#   flattened_image = tf.reshape(image, [-1, image_shape[2]])\n\n#   # Find all pixels where the last channel is zero.\n#   alpha_channel = tf.expand_dims(flattened_image[:, 3], axis=-1)\n\n#   replace = tf.concat([replace, tf.ones([1], image.dtype)], 0)\n\n#   # Where they are zero, fill them in with 'replace'.\n#   flattened_image = tf.where(\n#       tf.equal(alpha_channel, 0),\n#       tf.ones_like(flattened_image, dtype=image.dtype) * replace,\n#       flattened_image)\n\n#   image = tf.reshape(flattened_image, image_shape)\n#   image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n#   return image\n\n\n# def _randomly_negate_tensor(tensor):\n#   \"\"\"With 50% prob turn the tensor negative.\"\"\"\n#   should_flip = tf.cast(tf.floor(tf.random.uniform([]) + 0.5), tf.bool)\n#   final_tensor = tf.cond(should_flip, lambda: tensor, lambda: -tensor)\n#   return final_tensor\n\n\n# def _rotate_level_to_arg(level: float):\n#   level = (level / _MAX_LEVEL) * 30.\n#   level = _randomly_negate_tensor(level)\n#   return (level,)\n\n\n# def _shrink_level_to_arg(level: float):\n#   \"\"\"Converts level to ratio by which we shrink the image content.\"\"\"\n#   if level == 0:\n#     return (1.0,)  # if level is zero, do not shrink the image\n#   # Maximum shrinking ratio is 2.9.\n#   level = 2. / (_MAX_LEVEL / level) + 0.9\n#   return (level,)\n\n\n# def _enhance_level_to_arg(level: float):\n#   return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\n# def _shear_level_to_arg(level: float):\n#   level = (level / _MAX_LEVEL) * 0.3\n#   # Flip level to negative with 50% chance.\n#   level = _randomly_negate_tensor(level)\n#   return (level,)\n\n\n# def _translate_level_to_arg(level: float, translate_const: float):\n#   level = (level / _MAX_LEVEL) * float(translate_const)\n#   # Flip level to negative with 50% chance.\n#   level = _randomly_negate_tensor(level)\n#   return (level,)\n\n\n# def _mult_to_arg(level: float, multiplier: float = 1.):\n#   return (int((level / _MAX_LEVEL) * multiplier),)\n\n\n# def _apply_func_with_prob(func: Any, image: tf.Tensor, args: Any, prob: float):\n#   \"\"\"Apply `func` to image w/ `args` as input with probability `prob`.\"\"\"\n#   assert isinstance(args, tuple)\n\n#   # Apply the function with probability `prob`.\n#   should_apply_op = tf.cast(\n#       tf.floor(tf.random.uniform([], dtype=tf.float32) + prob), tf.bool)\n#   augmented_image = tf.cond(should_apply_op, lambda: func(image, *args),\n#                             lambda: image)\n#   return augmented_image\n\n\n# def select_and_apply_random_policy(policies: Any, image: tf.Tensor):\n#   \"\"\"Select a random policy from `policies` and apply it to `image`.\"\"\"\n#   policy_to_select = tf.random.uniform([], maxval=len(policies), dtype=tf.int32)\n#   # Note that using tf.case instead of tf.conds would result in significantly\n#   # larger graphs and would even break export for some larger policies.\n#   for (i, policy) in enumerate(policies):\n#     image = tf.cond(\n#         tf.equal(i, policy_to_select),\n#         lambda selected_policy=policy: selected_policy(image),\n#         lambda: image)\n#   return image\n\n\n# NAME_TO_FUNC = {\n#     'AutoContrast': autocontrast,\n#     'Equalize': equalize,\n#     'Invert': invert,\n#     'Rotate': wrapped_rotate,\n#     'Posterize': posterize,\n#     'Solarize': solarize,\n#     'SolarizeAdd': solarize_add,\n#     'Color': color,\n#     'Contrast': contrast,\n#     'Brightness': brightness,\n#     'Sharpness': sharpness,\n#     'ShearX': shear_x,\n#     'ShearY': shear_y,\n#     'TranslateX': translate_x,\n#     'TranslateY': translate_y,\n#     'Cutout': cutout,\n# }\n\n# # Functions that have a 'replace' parameter\n# REPLACE_FUNCS = frozenset({\n#     'Rotate',\n#     'TranslateX',\n#     'ShearX',\n#     'ShearY',\n#     'TranslateY',\n#     'Cutout',\n# })\n\n\n# def level_to_arg(cutout_const: float, translate_const: float):\n#   \"\"\"Creates a dict mapping image operation names to their arguments.\"\"\"\n\n#   no_arg = lambda level: ()\n#   posterize_arg = lambda level: _mult_to_arg(level, 4)\n#   solarize_arg = lambda level: _mult_to_arg(level, 256)\n#   solarize_add_arg = lambda level: _mult_to_arg(level, 110)\n#   cutout_arg = lambda level: _mult_to_arg(level, cutout_const)\n#   translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n\n#   args = {\n#       'AutoContrast': no_arg,\n#       'Equalize': no_arg,\n#       'Invert': no_arg,\n#       'Rotate': _rotate_level_to_arg,\n#       'Posterize': posterize_arg,\n#       'Solarize': solarize_arg,\n#       'SolarizeAdd': solarize_add_arg,\n#       'Color': _enhance_level_to_arg,\n#       'Contrast': _enhance_level_to_arg,\n#       'Brightness': _enhance_level_to_arg,\n#       'Sharpness': _enhance_level_to_arg,\n#       'ShearX': _shear_level_to_arg,\n#       'ShearY': _shear_level_to_arg,\n#       'Cutout': cutout_arg,\n#       'TranslateX': translate_arg,\n#       'TranslateY': translate_arg,\n#   }\n#   return args\n\n\n# def _parse_policy_info(name: Text, prob: float, level: float,\n#                        replace_value: List[int], cutout_const: float,\n#                        translate_const: float) -> Tuple[Any, float, Any]:\n#   \"\"\"Return the function that corresponds to `name` and update `level` param.\"\"\"\n#   func = NAME_TO_FUNC[name]\n#   args = level_to_arg(cutout_const, translate_const)[name](level)\n\n#   if name in REPLACE_FUNCS:\n#     # Add in replace arg if it is required for the function that is called.\n#     args = tuple(list(args) + [replace_value])\n\n#   return func, prob, args\n\n\n# class ImageAugment(object):\n#   \"\"\"Image augmentation class for applying image distortions.\"\"\"\n\n#   def distort(self, image: tf.Tensor) -> tf.Tensor:\n#     \"\"\"Given an image tensor, returns a distorted image with the same shape.\n\n#     Args:\n#       image: `Tensor` of shape [height, width, 3] representing an image.\n\n#     Returns:\n#       The augmented version of `image`.\n#     \"\"\"\n#     raise NotImplementedError()\n\n\n# class AutoAugment(ImageAugment):\n#   \"\"\"Applies the AutoAugment policy to images.\n\n#     AutoAugment is from the paper: https://arxiv.org/abs/1805.09501.\n#   \"\"\"\n\n#   def __init__(self,\n#                augmentation_name: Text = 'v0',\n#                policies: Optional[Dict[Text, Any]] = None,\n#                cutout_const: float = 100,\n#                translate_const: float = 250):\n#     \"\"\"Applies the AutoAugment policy to images.\n\n#     Args:\n#       augmentation_name: The name of the AutoAugment policy to use. The\n#         available options are `v0` and `test`. `v0` is the policy used for all\n#         of the results in the paper and was found to achieve the best results on\n#         the COCO dataset. `v1`, `v2` and `v3` are additional good policies found\n#         on the COCO dataset that have slight variation in what operations were\n#         used during the search procedure along with how many operations are\n#         applied in parallel to a single image (2 vs 3).\n#       policies: list of lists of tuples in the form `(func, prob, level)`,\n#         `func` is a string name of the augmentation function, `prob` is the\n#         probability of applying the `func` operation, `level` is the input\n#         argument for `func`.\n#       cutout_const: multiplier for applying cutout.\n#       translate_const: multiplier for applying translation.\n#     \"\"\"\n#     super(AutoAugment, self).__init__()\n\n#     if policies is None:\n#       self.available_policies = {\n#           'v0': self.policy_v0(),\n#           'test': self.policy_test(),\n#           'simple': self.policy_simple(),\n#       }\n\n#     if augmentation_name not in self.available_policies:\n#       raise ValueError(\n#           'Invalid augmentation_name: {}'.format(augmentation_name))\n\n#     self.augmentation_name = augmentation_name\n#     self.policies = self.available_policies[augmentation_name]\n#     self.cutout_const = float(cutout_const)\n#     self.translate_const = float(translate_const)\n\n#   def distort(self, image: tf.Tensor) -> tf.Tensor:\n#     \"\"\"Applies the AutoAugment policy to `image`.\n\n#     AutoAugment is from the paper: https://arxiv.org/abs/1805.09501.\n\n#     Args:\n#       image: `Tensor` of shape [height, width, 3] representing an image.\n\n#     Returns:\n#       A version of image that now has data augmentation applied to it based on\n#       the `policies` pass into the function.\n#     \"\"\"\n#     input_image_type = image.dtype\n\n#     if input_image_type != tf.uint8:\n#       image = tf.clip_by_value(image, 0.0, 255.0)\n#       image = tf.cast(image, dtype=tf.uint8)\n\n#     replace_value = [128] * 3\n\n#     # func is the string name of the augmentation function, prob is the\n#     # probability of applying the operation and level is the parameter\n#     # associated with the tf op.\n\n#     # tf_policies are functions that take in an image and return an augmented\n#     # image.\n#     tf_policies = []\n#     for policy in self.policies:\n#       tf_policy = []\n#       # Link string name to the correct python function and make sure the\n#       # correct argument is passed into that function.\n#       for policy_info in policy:\n#         policy_info = list(policy_info) + [\n#             replace_value, self.cutout_const, self.translate_const\n#         ]\n#         tf_policy.append(_parse_policy_info(*policy_info))\n#       # Now build the tf policy that will apply the augmentation procedue\n#       # on image.\n#       def make_final_policy(tf_policy_):\n\n#         def final_policy(image_):\n#           for func, prob, args in tf_policy_:\n#             image_ = _apply_func_with_prob(func, image_, args, prob)\n#           return image_\n\n#         return final_policy\n\n#       tf_policies.append(make_final_policy(tf_policy))\n\n#     image = select_and_apply_random_policy(tf_policies, image)\n#     image = tf.cast(image, dtype=input_image_type)\n#     return image\n\n#   @staticmethod\n#   def policy_v0():\n#     \"\"\"Autoaugment policy that was used in AutoAugment Paper.\n\n#     Each tuple is an augmentation operation of the form\n#     (operation, probability, magnitude). Each element in policy is a\n#     sub-policy that will be applied sequentially on the image.\n\n#     Returns:\n#       the policy.\n#     \"\"\"\n\n#     # TODO(dankondratyuk): tensorflow_addons defines custom ops, which\n#     # for some reason are not included when building/linking\n#     # This results in the error, \"Op type not registered\n#     # 'Addons>ImageProjectiveTransformV2' in binary\" when running on borg TPUs\n#     policy = [\n#         [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n#         [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n#         [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n#         [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n#         [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n#         [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n#         [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n#         [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)],\n#         [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n#         [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n#         [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)],\n#         [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n#         [('Posterize', 0.4, 6), ('AutoContrast', 0.4, 7)],\n#         [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n#         [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)],\n#         [('Rotate', 1.0, 7), ('TranslateY', 0.8, 9)],\n#         [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)],\n#         [('ShearY', 0.8, 0), ('Color', 0.6, 4)],\n#         [('Color', 1.0, 0), ('Rotate', 0.6, 2)],\n#         [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n#         [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n#         [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)],\n#         [('Posterize', 0.8, 2), ('Solarize', 0.6, 10)],\n#         [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n#         [('Color', 0.8, 6), ('Rotate', 0.4, 5)],\n#     ]\n#     return policy\n\n#   @staticmethod\n#   def policy_simple():\n#     \"\"\"Same as `policy_v0`, except with custom ops removed.\"\"\"\n\n#     policy = [\n#         [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n#         [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n#         [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n#         [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n#         [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n#         [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n#         [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n#         [('Posterize', 0.4, 6), ('AutoContrast', 0.4, 7)],\n#         [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n#         [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n#         [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n#         [('Posterize', 0.8, 2), ('Solarize', 0.6, 10)],\n#         [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n#     ]\n#     return policy\n\n#   @staticmethod\n#   def policy_test():\n#     \"\"\"Autoaugment test policy for debugging.\"\"\"\n#     policy = [\n#         [('TranslateX', 1.0, 4), ('Equalize', 1.0, 10)],\n#     ]\n#     return policy\n\n\n# class RandAugment(ImageAugment):\n#   \"\"\"Applies the RandAugment policy to images.\n\n#   RandAugment is from the paper https://arxiv.org/abs/1909.13719,\n#   \"\"\"\n\n#   def __init__(self,\n#                num_layers: int = 2,\n#                magnitude: float = 10.,\n#                cutout_const: float = 40.,\n#                translate_const: float = 100.):\n#     \"\"\"Applies the RandAugment policy to images.\n\n#     Args:\n#       num_layers: Integer, the number of augmentation transformations to apply\n#         sequentially to an image. Represented as (N) in the paper. Usually best\n#         values will be in the range [1, 3].\n#       magnitude: Integer, shared magnitude across all augmentation operations.\n#         Represented as (M) in the paper. Usually best values are in the range\n#         [5, 10].\n#       cutout_const: multiplier for applying cutout.\n#       translate_const: multiplier for applying translation.\n#     \"\"\"\n#     super(RandAugment, self).__init__()\n\n#     self.num_layers = num_layers\n#     self.magnitude = float(magnitude)\n#     self.cutout_const = float(cutout_const)\n#     self.translate_const = float(translate_const)\n#     self.available_ops = [\n#         'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize', 'Solarize',\n#         'Color', 'Contrast', 'Brightness', 'Sharpness', 'ShearX', 'ShearY',\n#         'TranslateX', 'TranslateY', 'Cutout', 'SolarizeAdd'\n#     ]\n\n#   def distort(self, image: tf.Tensor) -> tf.Tensor:\n#     \"\"\"Applies the RandAugment policy to `image`.\n\n#     Args:\n#       image: `Tensor` of shape [height, width, 3] representing an image.\n\n#     Returns:\n#       The augmented version of `image`.\n#     \"\"\"\n#     input_image_type = image.dtype\n\n#     if input_image_type != tf.uint8:\n#       image = tf.clip_by_value(image, 0.0, 255.0)\n#       image = tf.cast(image, dtype=tf.uint8)\n\n#     replace_value = [128] * 3\n#     min_prob, max_prob = 0.2, 0.8\n\n#     for _ in range(self.num_layers):\n#       op_to_select = tf.random.uniform([],\n#                                        maxval=len(self.available_ops) + 1,\n#                                        dtype=tf.int32)\n\n#       branch_fns = []\n#       for (i, op_name) in enumerate(self.available_ops):\n#         prob = tf.random.uniform([],\n#                                  minval=min_prob,\n#                                  maxval=max_prob,\n#                                  dtype=tf.float32)\n#         func, _, args = _parse_policy_info(op_name, prob, self.magnitude,\n#                                            replace_value, self.cutout_const,\n#                                            self.translate_const)\n#         branch_fns.append((\n#             i,\n#             # pylint:disable=g-long-lambda\n#             lambda selected_func=func, selected_args=args: selected_func(\n#                 image, *selected_args)))\n#         # pylint:enable=g-long-lambda\n\n#       image = tf.switch_case(\n#           branch_index=op_to_select,\n#           branch_fns=branch_fns,\n#           default=lambda: tf.identity(image))\n\n#     image = tf.cast(image, dtype=input_image_type)\n#     return image'''\n\n# with open('/opt/conda/lib/python3.10/site-packages/official/vision/image_classification/augment.py', 'w') as f:\n#     f.write(file)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-27T13:33:54.029906Z","iopub.execute_input":"2024-07-27T13:33:54.030268Z","iopub.status.idle":"2024-07-27T13:33:54.070176Z","shell.execute_reply.started":"2024-07-27T13:33:54.030195Z","shell.execute_reply":"2024-07-27T13:33:54.069318Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"For GPU:","metadata":{}},{"cell_type":"code","source":"file = '''# coding=utf-8\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains the TFExampleDecoder its associated helper classes.\n\nThe TFExampleDecode is a DataDecoder used to decode TensorFlow Example protos.\nIn order to do so each requested item must be paired with one or more Example\nfeatures that are parsed to produce the Tensor-based manifestation of the item.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\nfrom tf_slim.data import data_decoder\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import image_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import parsing_ops\nfrom tensorflow.python.ops import sparse_ops\n# pylint:enable=g-direct-tensorflow-import\nimport tensorflow as tf\n\n@six.add_metaclass(abc.ABCMeta)\nclass ItemHandler(object):\n  \"\"\"Specifies the item-to-Features mapping for tf.parse_example.\n\n  An ItemHandler both specifies a list of Features used for parsing an Example\n  proto as well as a function that post-processes the results of Example\n  parsing.\n  \"\"\"\n\n  def __init__(self, keys):\n    \"\"\"Constructs the handler with the name of the tf.train.Feature keys to use.\n\n    Args:\n      keys: the name of the TensorFlow Example Feature.\n    \"\"\"\n    if not isinstance(keys, (tuple, list)):\n      keys = [keys]\n    self._keys = keys\n\n  @property\n  def keys(self):\n    return self._keys\n\n  @abc.abstractmethod\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to the requested item.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      the final tensor representing the item being handled.\n    \"\"\"\n    pass\n\n\nclass ItemHandlerCallback(ItemHandler):\n  \"\"\"An ItemHandler that converts the parsed tensors via a given function.\n\n  Unlike other ItemHandlers, the ItemHandlerCallback resolves its item via\n  a callback function rather than using prespecified behavior.\n  \"\"\"\n\n  def __init__(self, keys, func):\n    \"\"\"Initializes the ItemHandler.\n\n    Args:\n      keys: a list of TF-Example keys.\n      func: a function that takes as an argument a dictionary from `keys` to\n        parsed Tensors.\n    \"\"\"\n    super(ItemHandlerCallback, self).__init__(keys)\n    self._func = func\n\n  def tensors_to_item(self, keys_to_tensors):\n    return self._func(keys_to_tensors)\n\n\nclass BoundingBox(ItemHandler):\n  \"\"\"An ItemHandler that concatenates a set of parsed Tensors to Bounding Boxes.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=''):\n    \"\"\"Initialize the bounding box handler.\n\n    Args:\n      keys: A list of four key names representing the ymin, xmin, ymax, mmax\n      prefix: An optional prefix for each of the bounding box keys.\n        If provided, `prefix` is appended to each key in `keys`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 4 keys\n    \"\"\"\n    if keys is None:\n      keys = ['ymin', 'xmin', 'ymax', 'xmax']\n    elif len(keys) != 4:\n      raise ValueError('BoundingBox expects 4 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    super(BoundingBox, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [num_boxes, 4] tensor of bounding box coordinates,\n        i.e. 1 bounding box per row, in order [y_min, x_min, y_max, x_max].\n    \"\"\"\n    sides = []\n    for key in self._full_keys:\n      side = keys_to_tensors[key]\n      if isinstance(side, sparse_tensor.SparseTensor):\n        side = side.values\n      side = array_ops.expand_dims(side, 0)\n      sides.append(side)\n\n    bounding_box = array_ops.concat(sides, 0)\n    return array_ops.transpose(bounding_box)\n\n\nclass Tensor(ItemHandler):\n  \"\"\"An ItemHandler that returns a parsed Tensor.\"\"\"\n\n  def __init__(self, tensor_key, shape_keys=None, shape=None, default_value=0):\n    \"\"\"Initializes the Tensor handler.\n\n    Tensors are, by default, returned without any reshaping. However, there are\n    two mechanisms which allow reshaping to occur at load time. If `shape_keys`\n    is provided, both the `Tensor` corresponding to `tensor_key` and\n    `shape_keys` is loaded and the former `Tensor` is reshaped with the values\n    of the latter. Alternatively, if a fixed `shape` is provided, the `Tensor`\n    corresponding to `tensor_key` is loaded and reshape appropriately.\n    If neither `shape_keys` nor `shape` are provided, the `Tensor` will be\n    returned without any reshaping.\n\n    Args:\n      tensor_key: the name of the `TFExample` feature to read the tensor from.\n      shape_keys: Optional name or list of names of the TF-Example feature in\n        which the tensor shape is stored. If a list, then each corresponds to\n        one dimension of the shape.\n      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n        reshaped accordingly.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if both `shape_keys` and `shape` are specified.\n    \"\"\"\n    if shape_keys and shape is not None:\n      raise ValueError('Cannot specify both shape_keys and shape parameters.')\n    if shape_keys and not isinstance(shape_keys, list):\n      shape_keys = [shape_keys]\n    self._tensor_key = tensor_key\n    self._shape_keys = shape_keys\n    self._shape = shape\n    self._default_value = default_value\n    keys = [tensor_key]\n    if shape_keys:\n      keys.extend(shape_keys)\n    super(Tensor, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    tensor = keys_to_tensors[self._tensor_key]\n    shape = self._shape\n    if self._shape_keys:\n      shape_dims = []\n      for k in self._shape_keys:\n        shape_dim = keys_to_tensors[k]\n        if isinstance(shape_dim, sparse_tensor.SparseTensor):\n          shape_dim = sparse_ops.sparse_tensor_to_dense(shape_dim)\n        shape_dims.append(shape_dim)\n      shape = array_ops.reshape(array_ops.stack(shape_dims), [-1])\n    if isinstance(tensor, sparse_tensor.SparseTensor):\n      if shape is not None:\n        tensor = sparse_ops.sparse_reshape(tensor, shape)\n      tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n    else:\n      if shape is not None:\n        tensor = array_ops.reshape(tensor, shape)\n    return tensor\n\n\nclass LookupTensor(Tensor):\n  \"\"\"An ItemHandler that returns a parsed Tensor, the result of a lookup.\"\"\"\n\n  def __init__(self,\n               tensor_key,\n               table,\n               shape_keys=None,\n               shape=None,\n               default_value=''):\n    \"\"\"Initializes the LookupTensor handler.\n\n    See Tensor.  Simply calls a vocabulary (most often, a label mapping) lookup.\n\n    Args:\n      tensor_key: the name of the `TFExample` feature to read the tensor from.\n      table: A tf.lookup table.\n      shape_keys: Optional name or list of names of the TF-Example feature in\n        which the tensor shape is stored. If a list, then each corresponds to\n        one dimension of the shape.\n      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n        reshaped accordingly.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if both `shape_keys` and `shape` are specified.\n    \"\"\"\n    self._table = table\n    super(LookupTensor, self).__init__(tensor_key, shape_keys, shape,\n                                       default_value)\n\n  def tensors_to_item(self, keys_to_tensors):\n    unmapped_tensor = super(LookupTensor, self).tensors_to_item(keys_to_tensors)\n    return self._table.lookup(unmapped_tensor)\n\n\nclass BackupHandler(ItemHandler):\n  \"\"\"An ItemHandler that tries two ItemHandlers in order.\"\"\"\n\n  def __init__(self, handler, backup):\n    \"\"\"Initializes the BackupHandler handler.\n\n    If the first Handler's tensors_to_item returns a Tensor with no elements,\n    the second Handler is used.\n\n    Args:\n      handler: The primary ItemHandler.\n      backup: The backup ItemHandler.\n\n    Raises:\n      ValueError: if either is not an ItemHandler.\n    \"\"\"\n    if not isinstance(handler, ItemHandler):\n      raise ValueError('Primary handler is of type %s instead of ItemHandler'\n                       % type(handler))\n    if not isinstance(backup, ItemHandler):\n      raise ValueError('Backup handler is of type %s instead of ItemHandler'\n                       % type(backup))\n    self._handler = handler\n    self._backup = backup\n    super(BackupHandler, self).__init__(handler.keys + backup.keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    item = self._handler.tensors_to_item(keys_to_tensors)\n    return tf.cond(\n        pred=math_ops.equal(math_ops.reduce_prod(array_ops.shape(item)), 0),\n        true_fn=lambda: self._backup.tensors_to_item(keys_to_tensors),\n        false_fn=lambda: item)\n\n\nclass SparseTensor(ItemHandler):\n  \"\"\"An ItemHandler for SparseTensors.\"\"\"\n\n  def __init__(self,\n               indices_key=None,\n               values_key=None,\n               shape_key=None,\n               shape=None,\n               densify=False,\n               default_value=0):\n    \"\"\"Initializes the Tensor handler.\n\n    Args:\n      indices_key: the name of the TF-Example feature that contains the ids.\n        Defaults to 'indices'.\n      values_key: the name of the TF-Example feature that contains the values.\n        Defaults to 'values'.\n      shape_key: the name of the TF-Example feature that contains the shape.\n        If provided it would be used.\n      shape: the output shape of the SparseTensor. If `shape_key` is not\n        provided this `shape` would be used.\n      densify: whether to convert the SparseTensor into a dense Tensor.\n      default_value: Scalar value to set when making dense for indices not\n        specified in the `SparseTensor`.\n    \"\"\"\n    indices_key = indices_key or 'indices'\n    values_key = values_key or 'values'\n    self._indices_key = indices_key\n    self._values_key = values_key\n    self._shape_key = shape_key\n    self._shape = shape\n    self._densify = densify\n    self._default_value = default_value\n    keys = [indices_key, values_key]\n    if shape_key:\n      keys.append(shape_key)\n    super(SparseTensor, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    indices = keys_to_tensors[self._indices_key]\n    values = keys_to_tensors[self._values_key]\n    if self._shape_key:\n      shape = keys_to_tensors[self._shape_key]\n      if isinstance(shape, sparse_tensor.SparseTensor):\n        shape = sparse_ops.sparse_tensor_to_dense(shape)\n    elif self._shape:\n      shape = self._shape\n    else:\n      shape = indices.dense_shape\n    indices_shape = array_ops.shape(indices.indices)\n    rank = indices_shape[1]\n    ids = math_ops.cast(indices.values, dtypes.int64)\n    indices_columns_to_preserve = array_ops.slice(\n        indices.indices, [0, 0], array_ops.stack([-1, rank - 1]))\n    new_indices = array_ops.concat(\n        [indices_columns_to_preserve, array_ops.reshape(ids, [-1, 1])], 1)\n\n    tensor = sparse_tensor.SparseTensor(new_indices, values.values, shape)\n    if self._densify:\n      tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n    return tensor\n\n\nclass Image(ItemHandler):\n  \"\"\"An ItemHandler that decodes a parsed Tensor as an image.\"\"\"\n\n  def __init__(self,\n               image_key=None,\n               format_key=None,\n               shape=None,\n               channels=3,\n               dtype=dtypes.uint8,\n               repeated=False,\n               dct_method=''):\n    \"\"\"Initializes the image.\n\n    Args:\n      image_key: the name of the TF-Example feature in which the encoded image\n        is stored.\n      format_key: the name of the TF-Example feature in which the image format\n        is stored.\n      shape: the output shape of the image as 1-D `Tensor`\n        [height, width, channels]. If provided, the image is reshaped\n        accordingly. If left as None, no reshaping is done. A shape should\n        be supplied only if all the stored images have the same shape.\n      channels: the number of channels in the image.\n      dtype: images will be decoded at this bit depth. Different formats\n        support different bit depths.\n          See tf.image.decode_image,\n              tf.io.decode_raw,\n      repeated: if False, decodes a single image. If True, decodes a\n        variable number of image strings from a 1D tensor of strings.\n      dct_method: An optional string. Defaults to empty string. It only takes\n        effect when image format is jpeg, used to specify a hint about the\n        algorithm used for jpeg decompression. Currently valid values\n        are ['INTEGER_FAST', 'INTEGER_ACCURATE']. The hint may be ignored, for\n        example, the jpeg library does not have that specific option.\n    \"\"\"\n    if not image_key:\n      image_key = 'image/encoded'\n    if not format_key:\n      format_key = 'image/format'\n\n    super(Image, self).__init__([image_key, format_key])\n    self._image_key = image_key\n    self._format_key = format_key\n    self._shape = shape\n    self._channels = channels\n    self._dtype = dtype\n    self._repeated = repeated\n    self._dct_method = dct_method\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"See base class.\"\"\"\n    image_buffer = keys_to_tensors[self._image_key]\n    image_format = keys_to_tensors[self._format_key]\n\n    if self._repeated:\n      return map_fn.map_fn(lambda x: self._decode(x, image_format),\n                           image_buffer, dtype=self._dtype)\n    else:\n      return self._decode(image_buffer, image_format)\n\n  def _decode(self, image_buffer, image_format):\n    \"\"\"Decodes the image buffer.\n\n    Args:\n      image_buffer: The tensor representing the encoded image tensor.\n      image_format: The image format for the image in `image_buffer`. If image\n        format is `raw`, all images are expected to be in this format, otherwise\n        this op can decode a mix of `jpg` and `png` formats.\n\n    Returns:\n      A tensor that represents decoded image of self._shape, or\n      (?, ?, self._channels) if self._shape is not specified.\n    \"\"\"\n\n    def decode_image():\n      \"\"\"Decodes a image based on the headers.\"\"\"\n      return math_ops.cast(\n          image_ops.decode_image(image_buffer, channels=self._channels),\n          self._dtype)\n\n    def decode_jpeg():\n      \"\"\"Decodes a jpeg image with specified '_dct_method'.\"\"\"\n      return math_ops.cast(\n          image_ops.decode_jpeg(\n              image_buffer,\n              channels=self._channels,\n              dct_method=self._dct_method), self._dtype)\n\n    def check_jpeg():\n      \"\"\"Checks if an image is jpeg.\"\"\"\n      # For jpeg, we directly use image_ops.decode_jpeg rather than decode_image\n      # in order to feed the jpeg specify parameter 'dct_method'.\n      return tf.cond(\n          image_ops.is_jpeg(image_buffer),\n          decode_jpeg,\n          decode_image,\n          name='cond_jpeg')\n\n    def decode_raw():\n      \"\"\"Decodes a raw image.\"\"\"\n      return parsing_ops.decode_raw(image_buffer, out_type=self._dtype)\n\n    pred_fn_pairs = [(math_ops.logical_or(\n        math_ops.equal(image_format, 'raw'),\n        math_ops.equal(image_format, 'RAW')), decode_raw)]\n\n    image = tf.case(\n        pred_fn_pairs, default=check_jpeg, exclusive=True)\n\n    image.set_shape([None, None, self._channels])\n    if self._shape is not None:\n      image = array_ops.reshape(image, self._shape)\n\n    return image\n\n\nclass BoundingBoxSequence(ItemHandler):\n  \"\"\"An ItemHandler that concatenates SparseTensors to Bounding Boxes.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=None, return_dense=True,\n               default_value=-1.0):\n    \"\"\"Initialize the bounding box handler.\n\n    Args:\n      keys: A list of four key names representing the ymin, xmin, ymax, xmax\n        in the Example or SequenceExample.\n      prefix: An optional prefix for each of the bounding box keys in the\n        Example or SequenceExample. If provided, `prefix` is prepended to each\n        key in `keys`.\n      return_dense: if True, returns a dense tensor; if False, returns as\n        sparse tensor.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 4 keys\n    \"\"\"\n    if keys is None:\n      keys = ['ymin', 'xmin', 'ymax', 'xmax']\n    elif len(keys) != 4:\n      raise ValueError('BoundingBoxSequence expects 4 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    self._return_dense = return_dense\n    self._default_value = default_value\n    super(BoundingBoxSequence, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time, num_boxes, 4] tensor of bounding box coordinates, in order\n          [y_min, x_min, y_max, x_max]. Whether the tensor is a SparseTensor\n          or a dense Tensor is determined by the return_dense parameter. Empty\n          positions in the sparse tensor are filled with -1.0 values.\n    \"\"\"\n    sides = []\n    for key in self._full_keys:\n      value = keys_to_tensors[key]\n      expanded_dims = array_ops.concat(\n          [math_ops.to_int64(array_ops.shape(value)),\n           constant_op.constant([1], dtype=dtypes.int64)], 0)\n      side = sparse_ops.sparse_reshape(value, expanded_dims)\n      sides.append(side)\n    bounding_boxes = sparse_ops.sparse_concat(2, sides)\n    if self._return_dense:\n      bounding_boxes = sparse_ops.sparse_tensor_to_dense(\n          bounding_boxes, default_value=self._default_value)\n    return bounding_boxes\n\n\nclass NumBoxesSequence(ItemHandler):\n  \"\"\"An ItemHandler that returns num_boxes per frame for a box sequence.\n\n  `num_boxes` is inferred from a 2D SparseTensor decoded from a field in the\n  SequenceExample. The SparseTensor is partially dense and only ragged along its\n  second dimensions.\n\n  The output is an int64 tf.Tensor of shape [time], which is solely determined\n  by the tensor of the first key. However, if `check_consistency` is True, this\n  function checks that `num_boxes` is consistent across all keys.\n  \"\"\"\n\n  def __init__(self, keys=None, check_consistency=True):\n    \"\"\"Initialization.\n\n    Args:\n      keys: A list of keys of sparse tensors which have exactly 2 dimensions,\n        with the 1st being the `time` and the 2nd the `boxes` per frame.\n        key in `keys`.\n      check_consistency: if True, check for consistency.\n\n    Raises:\n      ValueError: If keys is empty.\n    \"\"\"\n    if not keys:\n      raise ValueError('keys must not be empty.')\n    self._check_consistency = check_consistency\n    super(NumBoxesSequence, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a num_boxes tensor.\n\n    If check_consistency is True: raises runtime error in Tensorflow when the\n    consistency is violated across tensors.\n\n    Args:\n      keys_to_tensors: A mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time] tf.Tensor containing the number of boxes per frame.\n\n    Raises:\n      ValueError: If any of the keyed tensors is not sparse or exactly 2\n        dimensional.\n    \"\"\"\n    def _compute_num_boxes(tensor):\n      \"\"\"Compute num_boxes from a single 2D tensor.\"\"\"\n      if not isinstance(tensor, sparse_tensor.SparseTensor):\n        raise ValueError('tensor must be of type tf.SparseTensor.')\n      indices = tensor.indices\n      dense_shape = tensor.dense_shape\n      box_ids = indices[:, 1]\n      box_ids = sparse_tensor.SparseTensor(\n          indices=indices, values=box_ids, dense_shape=dense_shape)\n      box_ids = sparse_ops.sparse_tensor_to_dense(box_ids, default_value=-1)\n      # In the event that the parsed tensor is empty (perhaps due to a negative\n      # example), we pad box_ids so that the resulting number of boxes is 0.\n      num_boxes = math_ops.reduce_max(\n          array_ops.pad(box_ids + 1, [[0, 0], [0, 1]]), axis=1)\n      return num_boxes\n\n    num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[0]])\n    asserts = []\n    if self._check_consistency:\n      for i in range(1, len(self._keys)):\n        cur_num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[i]])\n        asserts.append(check_ops.assert_equal(num_boxes, cur_num_boxes))\n\n    with ops.control_dependencies(asserts):\n      return array_ops.identity(num_boxes)\n\n\nclass KeypointsSequence(ItemHandler):\n  \"\"\"An ItemHandler that concatenates SparseTensors to Keypoints.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=None, return_dense=True,\n               default_value=-1.0):\n    \"\"\"Initialize the keypoints handler.\n\n    Args:\n      keys: A list of two key names representing the y and x coordinates in the\n        Example or SequenceExample.\n      prefix: An optional prefix for each of the keypoint keys in the Example\n        or SequenceExample. If provided, `prefix` is prepended to each key in\n        `keys`.\n      return_dense: if True, returns a dense tensor; if False, returns as\n        sparse tensor.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 2 keys\n    \"\"\"\n    if keys is None:\n      keys = ['y', 'x']\n    elif len(keys) != 2:\n      raise ValueError('KeypointsSequence expects 2 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    self._return_dense = return_dense\n    self._default_value = default_value\n    super(KeypointsSequence, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of keypoints.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time, num_keypoints, 2] tensor of keypoint coordinates, in order [y, x].\n          Whether the tensor is a SparseTensor or a dense Tensor is determined\n          by the return_dense parameter. Empty positions in the sparse tensor\n          are filled with -1.0 values.\n    \"\"\"\n    coordinates = []\n    for key in self._full_keys:\n      value = keys_to_tensors[key]\n      expanded_dims = array_ops.concat(\n          [math_ops.to_int64(array_ops.shape(value)),\n           constant_op.constant([1], dtype=dtypes.int64)], 0)\n      coordinate = sparse_ops.sparse_reshape(value, expanded_dims)\n      coordinates.append(coordinate)\n    keypoints = sparse_ops.sparse_concat(2, coordinates)\n    if self._return_dense:\n      keypoints = sparse_ops.sparse_tensor_to_dense(\n          keypoints, default_value=self._default_value)\n    return keypoints\n\n\nclass TFExampleDecoder(data_decoder.DataDecoder):\n  \"\"\"A decoder for TensorFlow Examples.\n\n  Decoding Example proto buffers is comprised of two stages: (1) Example parsing\n  and (2) tensor manipulation.\n\n  In the first stage, the tf.io.parse_example function is called with a list of\n  FixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\n  the example. The output of this stage is a set of tensors.\n\n  In the second stage, the resulting tensors are manipulated to provide the\n  requested 'item' tensors.\n\n  To perform this decoding operation, an ExampleDecoder is given a list of\n  ItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\n  contains the instructions for post_processing its tensors for stage 2.\n  \"\"\"\n\n  def __init__(self, keys_to_features, items_to_handlers):\n    \"\"\"Constructs the decoder.\n\n    Args:\n      keys_to_features: a dictionary from TF-Example keys to either\n        tf.io.VarLenFeature or tf.io.FixedLenFeature instances. See tensorflow's\n        parsing_ops.py.\n      items_to_handlers: a dictionary from items (strings) to ItemHandler\n        instances. Note that the ItemHandler's are provided the keys that they\n        use to return the final item Tensors.\n    \"\"\"\n    self._keys_to_features = keys_to_features\n    self._items_to_handlers = items_to_handlers\n\n  def list_items(self):\n    \"\"\"See base class.\"\"\"\n    return list(self._items_to_handlers.keys())\n\n  def decode(self, serialized_example, items=None):\n    \"\"\"Decodes the given serialized TF-example.\n\n    Args:\n      serialized_example: a serialized TF-example tensor.\n      items: the list of items to decode. These must be a subset of the item\n        keys in self._items_to_handlers. If `items` is left as None, then all\n        of the items in self._items_to_handlers are decoded.\n\n    Returns:\n      the decoded items, a list of tensor.\n    \"\"\"\n    example = parsing_ops.parse_single_example(serialized_example,\n                                               self._keys_to_features)\n\n    # Reshape non-sparse elements just once, adding the reshape ops in\n    # deterministic order.\n    for k in sorted(self._keys_to_features):\n      v = self._keys_to_features[k]\n      if isinstance(v, parsing_ops.FixedLenFeature):\n        example[k] = array_ops.reshape(example[k], v.shape)\n\n    if not items:\n      items = self._items_to_handlers.keys()\n\n    outputs = []\n    for item in items:\n      handler = self._items_to_handlers[item]\n      keys_to_tensors = {key: example[key] for key in handler.keys}\n      outputs.append(handler.tensors_to_item(keys_to_tensors))\n    return outputs\n\n\nclass TFSequenceExampleDecoder(data_decoder.DataDecoder):\n  \"\"\"A decoder for TensorFlow SequenceExamples.\n\n  Decoding SequenceExample proto buffers is comprised of two stages:\n  (1) Example parsing and (2) tensor manipulation.\n\n  In the first stage, the tf.parse_single_sequence_example function is called\n  with a list of FixedLenFeatures and SparseLenFeatures. These instances tell TF\n  how to parse the example. The output of this stage is a set of tensors.\n\n  In the second stage, the resulting tensors are manipulated to provide the\n  requested 'item' tensors.\n\n  To perform this decoding operation, a SequenceExampleDecoder is given a list\n  of ItemHandlers. Each ItemHandler indicates the set of features for stage 1\n  and contains the instructions for post_processing its tensors for stage 2.\n  \"\"\"\n\n  def __init__(self, keys_to_context_features, keys_to_sequence_features,\n               items_to_handlers):\n    \"\"\"Constructs the decoder.\n\n    Args:\n      keys_to_context_features: a dictionary from TF-SequenceExample context\n        keys to either tf.VarLenFeature or tf.FixedLenFeature instances.\n        See tensorflow's parsing_ops.py.\n      keys_to_sequence_features: a dictionary from TF-SequenceExample sequence\n        keys to either tf.VarLenFeature or tf.FixedLenSequenceFeature instances.\n        See tensorflow's parsing_ops.py.\n      items_to_handlers: a dictionary from items (strings) to ItemHandler\n        instances. Note that the ItemHandler's are provided the keys that they\n        use to return the final item Tensors.\n\n    Raises:\n      ValueError: if the same key is present for context features and sequence\n        features.\n    \"\"\"\n    unique_keys = set()\n    unique_keys.update(keys_to_context_features)\n    unique_keys.update(keys_to_sequence_features)\n\n    self._keys_to_context_features = keys_to_context_features\n    self._keys_to_sequence_features = keys_to_sequence_features\n    self._items_to_handlers = items_to_handlers\n\n  def list_items(self):\n    \"\"\"See base class.\"\"\"\n    return self._items_to_handlers.keys()\n\n  def decode(self, serialized_example, items=None):\n    \"\"\"Decodes the given serialized TF-SequenceExample.\n\n    Args:\n      serialized_example: a serialized TF-SequenceExample tensor.\n      items: the list of items to decode. These must be a subset of the item\n        keys in self._items_to_handlers. If `items` is left as None, then all\n        of the items in self._items_to_handlers are decoded.\n\n    Returns:\n      the decoded items, a list of tensor.\n    \"\"\"\n\n    context, feature_list = parsing_ops.parse_single_sequence_example(\n        serialized_example, self._keys_to_context_features,\n        self._keys_to_sequence_features)\n\n    # Reshape non-sparse elements just once:\n    for k in self._keys_to_context_features:\n      v = self._keys_to_context_features[k]\n      if isinstance(v, parsing_ops.FixedLenFeature):\n        context[k] = array_ops.reshape(context[k], v.shape)\n\n    if not items:\n      items = self._items_to_handlers.keys()\n\n    outputs = []\n    for item in items:\n      handler = self._items_to_handlers[item]\n      keys_to_tensors = {\n          key: context[key] if key in context else feature_list[key]\n          for key in handler.keys\n      }\n      outputs.append(handler.tensors_to_item(keys_to_tensors))\n    return outputs'''\n\nwith open('/opt/conda/lib/python3.10/site-packages/tf_slim/data/tfexample_decoder.py', 'w') as f:\n    f.write(file)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:54.071846Z","iopub.execute_input":"2024-07-27T13:33:54.072323Z","iopub.status.idle":"2024-07-27T13:33:54.106656Z","shell.execute_reply.started":"2024-07-27T13:33:54.072290Z","shell.execute_reply":"2024-07-27T13:33:54.105809Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"For TPU:","metadata":{}},{"cell_type":"code","source":"# file = '''# coding=utf-8\n# # Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n# #\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# # ==============================================================================\n# \"\"\"Contains the TFExampleDecoder its associated helper classes.\n\n# The TFExampleDecode is a DataDecoder used to decode TensorFlow Example protos.\n# In order to do so each requested item must be paired with one or more Example\n# features that are parsed to produce the Tensor-based manifestation of the item.\n# \"\"\"\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\n# import abc\n\n# import six\n# from tf_slim.data import data_decoder\n# # pylint:disable=g-direct-tensorflow-import\n# from tensorflow.python.framework import constant_op\n# from tensorflow.python.framework import dtypes\n# from tensorflow.python.framework import ops\n# from tensorflow.python.framework import sparse_tensor\n# from tensorflow.python.ops import array_ops\n# from tensorflow.python.ops import check_ops\n# from tensorflow.python.ops import control_flow_ops\n# from tensorflow.python.ops import image_ops\n# from tensorflow.python.ops import map_fn\n# from tensorflow.python.ops import math_ops\n# from tensorflow.python.ops import parsing_ops\n# from tensorflow.python.ops import sparse_ops\n# # pylint:enable=g-direct-tensorflow-import\n# import tensorflow as tf\n\n# @six.add_metaclass(abc.ABCMeta)\n# class ItemHandler(object):\n#   \"\"\"Specifies the item-to-Features mapping for tf.parse_example.\n\n#   An ItemHandler both specifies a list of Features used for parsing an Example\n#   proto as well as a function that post-processes the results of Example\n#   parsing.\n#   \"\"\"\n\n#   def __init__(self, keys):\n#     \"\"\"Constructs the handler with the name of the tf.train.Feature keys to use.\n\n#     Args:\n#       keys: the name of the TensorFlow Example Feature.\n#     \"\"\"\n#     if not isinstance(keys, (tuple, list)):\n#       keys = [keys]\n#     self._keys = keys\n\n#   @property\n#   def keys(self):\n#     return self._keys\n\n#   @abc.abstractmethod\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"Maps the given dictionary of tensors to the requested item.\n\n#     Args:\n#       keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n#     Returns:\n#       the final tensor representing the item being handled.\n#     \"\"\"\n#     pass\n\n\n# class ItemHandlerCallback(ItemHandler):\n#   \"\"\"An ItemHandler that converts the parsed tensors via a given function.\n\n#   Unlike other ItemHandlers, the ItemHandlerCallback resolves its item via\n#   a callback function rather than using prespecified behavior.\n#   \"\"\"\n\n#   def __init__(self, keys, func):\n#     \"\"\"Initializes the ItemHandler.\n\n#     Args:\n#       keys: a list of TF-Example keys.\n#       func: a function that takes as an argument a dictionary from `keys` to\n#         parsed Tensors.\n#     \"\"\"\n#     super(ItemHandlerCallback, self).__init__(keys)\n#     self._func = func\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     return self._func(keys_to_tensors)\n\n\n# class BoundingBox(ItemHandler):\n#   \"\"\"An ItemHandler that concatenates a set of parsed Tensors to Bounding Boxes.\n#   \"\"\"\n\n#   def __init__(self, keys=None, prefix=''):\n#     \"\"\"Initialize the bounding box handler.\n\n#     Args:\n#       keys: A list of four key names representing the ymin, xmin, ymax, mmax\n#       prefix: An optional prefix for each of the bounding box keys.\n#         If provided, `prefix` is appended to each key in `keys`.\n\n#     Raises:\n#       ValueError: if keys is not `None` and also not a list of exactly 4 keys\n#     \"\"\"\n#     if keys is None:\n#       keys = ['ymin', 'xmin', 'ymax', 'xmax']\n#     elif len(keys) != 4:\n#       raise ValueError('BoundingBox expects 4 keys but got {}'.format(\n#           len(keys)))\n#     self._prefix = prefix\n#     self._keys = keys\n#     self._full_keys = [prefix + k for k in keys]\n#     super(BoundingBox, self).__init__(self._full_keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n#     Args:\n#       keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n#     Returns:\n#       [num_boxes, 4] tensor of bounding box coordinates,\n#         i.e. 1 bounding box per row, in order [y_min, x_min, y_max, x_max].\n#     \"\"\"\n#     sides = []\n#     for key in self._full_keys:\n#       side = keys_to_tensors[key]\n#       if isinstance(side, sparse_tensor.SparseTensor):\n#         side = side.values\n#       side = array_ops.expand_dims(side, 0)\n#       sides.append(side)\n\n#     bounding_box = array_ops.concat(sides, 0)\n#     return array_ops.transpose(bounding_box)\n\n\n# class Tensor(ItemHandler):\n#   \"\"\"An ItemHandler that returns a parsed Tensor.\"\"\"\n\n#   def __init__(self, tensor_key, shape_keys=None, shape=None, default_value=0):\n#     \"\"\"Initializes the Tensor handler.\n\n#     Tensors are, by default, returned without any reshaping. However, there are\n#     two mechanisms which allow reshaping to occur at load time. If `shape_keys`\n#     is provided, both the `Tensor` corresponding to `tensor_key` and\n#     `shape_keys` is loaded and the former `Tensor` is reshaped with the values\n#     of the latter. Alternatively, if a fixed `shape` is provided, the `Tensor`\n#     corresponding to `tensor_key` is loaded and reshape appropriately.\n#     If neither `shape_keys` nor `shape` are provided, the `Tensor` will be\n#     returned without any reshaping.\n\n#     Args:\n#       tensor_key: the name of the `TFExample` feature to read the tensor from.\n#       shape_keys: Optional name or list of names of the TF-Example feature in\n#         which the tensor shape is stored. If a list, then each corresponds to\n#         one dimension of the shape.\n#       shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n#         reshaped accordingly.\n#       default_value: The value used when the `tensor_key` is not found in a\n#         particular `TFExample`.\n\n#     Raises:\n#       ValueError: if both `shape_keys` and `shape` are specified.\n#     \"\"\"\n#     if shape_keys and shape is not None:\n#       raise ValueError('Cannot specify both shape_keys and shape parameters.')\n#     if shape_keys and not isinstance(shape_keys, list):\n#       shape_keys = [shape_keys]\n#     self._tensor_key = tensor_key\n#     self._shape_keys = shape_keys\n#     self._shape = shape\n#     self._default_value = default_value\n#     keys = [tensor_key]\n#     if shape_keys:\n#       keys.extend(shape_keys)\n#     super(Tensor, self).__init__(keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     tensor = keys_to_tensors[self._tensor_key]\n#     shape = self._shape\n#     if self._shape_keys:\n#       shape_dims = []\n#       for k in self._shape_keys:\n#         shape_dim = keys_to_tensors[k]\n#         if isinstance(shape_dim, sparse_tensor.SparseTensor):\n#           shape_dim = sparse_ops.sparse_tensor_to_dense(shape_dim)\n#         shape_dims.append(shape_dim)\n#       shape = array_ops.reshape(array_ops.stack(shape_dims), [-1])\n#     if isinstance(tensor, sparse_tensor.SparseTensor):\n#       if shape is not None:\n#         tensor = sparse_ops.sparse_reshape(tensor, shape)\n#       tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n#     else:\n#       if shape is not None:\n#         tensor = array_ops.reshape(tensor, shape)\n#     return tensor\n\n\n# class LookupTensor(Tensor):\n#   \"\"\"An ItemHandler that returns a parsed Tensor, the result of a lookup.\"\"\"\n\n#   def __init__(self,\n#                tensor_key,\n#                table,\n#                shape_keys=None,\n#                shape=None,\n#                default_value=''):\n#     \"\"\"Initializes the LookupTensor handler.\n\n#     See Tensor.  Simply calls a vocabulary (most often, a label mapping) lookup.\n\n#     Args:\n#       tensor_key: the name of the `TFExample` feature to read the tensor from.\n#       table: A tf.lookup table.\n#       shape_keys: Optional name or list of names of the TF-Example feature in\n#         which the tensor shape is stored. If a list, then each corresponds to\n#         one dimension of the shape.\n#       shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n#         reshaped accordingly.\n#       default_value: The value used when the `tensor_key` is not found in a\n#         particular `TFExample`.\n\n#     Raises:\n#       ValueError: if both `shape_keys` and `shape` are specified.\n#     \"\"\"\n#     self._table = table\n#     super(LookupTensor, self).__init__(tensor_key, shape_keys, shape,\n#                                        default_value)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     unmapped_tensor = super(LookupTensor, self).tensors_to_item(keys_to_tensors)\n#     return self._table.lookup(unmapped_tensor)\n\n\n# class BackupHandler(ItemHandler):\n#   \"\"\"An ItemHandler that tries two ItemHandlers in order.\"\"\"\n\n#   def __init__(self, handler, backup):\n#     \"\"\"Initializes the BackupHandler handler.\n\n#     If the first Handler's tensors_to_item returns a Tensor with no elements,\n#     the second Handler is used.\n\n#     Args:\n#       handler: The primary ItemHandler.\n#       backup: The backup ItemHandler.\n\n#     Raises:\n#       ValueError: if either is not an ItemHandler.\n#     \"\"\"\n#     if not isinstance(handler, ItemHandler):\n#       raise ValueError('Primary handler is of type %s instead of ItemHandler'\n#                        % type(handler))\n#     if not isinstance(backup, ItemHandler):\n#       raise ValueError('Backup handler is of type %s instead of ItemHandler'\n#                        % type(backup))\n#     self._handler = handler\n#     self._backup = backup\n#     super(BackupHandler, self).__init__(handler.keys + backup.keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     item = self._handler.tensors_to_item(keys_to_tensors)\n#     return tf.cond(\n#         pred=math_ops.equal(math_ops.reduce_prod(array_ops.shape(item)), 0),\n#         true_fn=lambda: self._backup.tensors_to_item(keys_to_tensors),\n#         false_fn=lambda: item)\n\n\n# class SparseTensor(ItemHandler):\n#   \"\"\"An ItemHandler for SparseTensors.\"\"\"\n\n#   def __init__(self,\n#                indices_key=None,\n#                values_key=None,\n#                shape_key=None,\n#                shape=None,\n#                densify=False,\n#                default_value=0):\n#     \"\"\"Initializes the Tensor handler.\n\n#     Args:\n#       indices_key: the name of the TF-Example feature that contains the ids.\n#         Defaults to 'indices'.\n#       values_key: the name of the TF-Example feature that contains the values.\n#         Defaults to 'values'.\n#       shape_key: the name of the TF-Example feature that contains the shape.\n#         If provided it would be used.\n#       shape: the output shape of the SparseTensor. If `shape_key` is not\n#         provided this `shape` would be used.\n#       densify: whether to convert the SparseTensor into a dense Tensor.\n#       default_value: Scalar value to set when making dense for indices not\n#         specified in the `SparseTensor`.\n#     \"\"\"\n#     indices_key = indices_key or 'indices'\n#     values_key = values_key or 'values'\n#     self._indices_key = indices_key\n#     self._values_key = values_key\n#     self._shape_key = shape_key\n#     self._shape = shape\n#     self._densify = densify\n#     self._default_value = default_value\n#     keys = [indices_key, values_key]\n#     if shape_key:\n#       keys.append(shape_key)\n#     super(SparseTensor, self).__init__(keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     indices = keys_to_tensors[self._indices_key]\n#     values = keys_to_tensors[self._values_key]\n#     if self._shape_key:\n#       shape = keys_to_tensors[self._shape_key]\n#       if isinstance(shape, sparse_tensor.SparseTensor):\n#         shape = sparse_ops.sparse_tensor_to_dense(shape)\n#     elif self._shape:\n#       shape = self._shape\n#     else:\n#       shape = indices.dense_shape\n#     indices_shape = array_ops.shape(indices.indices)\n#     rank = indices_shape[1]\n#     ids = math_ops.cast(indices.values, dtypes.int64)\n#     indices_columns_to_preserve = array_ops.slice(\n#         indices.indices, [0, 0], array_ops.stack([-1, rank - 1]))\n#     new_indices = array_ops.concat(\n#         [indices_columns_to_preserve, array_ops.reshape(ids, [-1, 1])], 1)\n\n#     tensor = sparse_tensor.SparseTensor(new_indices, values.values, shape)\n#     if self._densify:\n#       tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n#     return tensor\n\n\n# class Image(ItemHandler):\n#   \"\"\"An ItemHandler that decodes a parsed Tensor as an image.\"\"\"\n\n#   def __init__(self,\n#                image_key=None,\n#                format_key=None,\n#                shape=None,\n#                channels=3,\n#                dtype=dtypes.uint8,\n#                repeated=False,\n#                dct_method=''):\n#     \"\"\"Initializes the image.\n\n#     Args:\n#       image_key: the name of the TF-Example feature in which the encoded image\n#         is stored.\n#       format_key: the name of the TF-Example feature in which the image format\n#         is stored.\n#       shape: the output shape of the image as 1-D `Tensor`\n#         [height, width, channels]. If provided, the image is reshaped\n#         accordingly. If left as None, no reshaping is done. A shape should\n#         be supplied only if all the stored images have the same shape.\n#       channels: the number of channels in the image.\n#       dtype: images will be decoded at this bit depth. Different formats\n#         support different bit depths.\n#           See tf.image.decode_image,\n#               tf.io.decode_raw,\n#       repeated: if False, decodes a single image. If True, decodes a\n#         variable number of image strings from a 1D tensor of strings.\n#       dct_method: An optional string. Defaults to empty string. It only takes\n#         effect when image format is jpeg, used to specify a hint about the\n#         algorithm used for jpeg decompression. Currently valid values\n#         are ['INTEGER_FAST', 'INTEGER_ACCURATE']. The hint may be ignored, for\n#         example, the jpeg library does not have that specific option.\n#     \"\"\"\n#     if not image_key:\n#       image_key = 'image/encoded'\n#     if not format_key:\n#       format_key = 'image/format'\n\n#     super(Image, self).__init__([image_key, format_key])\n#     self._image_key = image_key\n#     self._format_key = format_key\n#     self._shape = shape\n#     self._channels = channels\n#     self._dtype = dtype\n#     self._repeated = repeated\n#     self._dct_method = dct_method\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"See base class.\"\"\"\n#     image_buffer = keys_to_tensors[self._image_key]\n#     image_format = keys_to_tensors[self._format_key]\n\n#     if self._repeated:\n#       return map_fn.map_fn(lambda x: self._decode(x, image_format),\n#                            image_buffer, dtype=self._dtype)\n#     else:\n#       return self._decode(image_buffer, image_format)\n\n#   def _decode(self, image_buffer, image_format):\n#     \"\"\"Decodes the image buffer.\n\n#     Args:\n#       image_buffer: The tensor representing the encoded image tensor.\n#       image_format: The image format for the image in `image_buffer`. If image\n#         format is `raw`, all images are expected to be in this format, otherwise\n#         this op can decode a mix of `jpg` and `png` formats.\n\n#     Returns:\n#       A tensor that represents decoded image of self._shape, or\n#       (?, ?, self._channels) if self._shape is not specified.\n#     \"\"\"\n\n#     def decode_image():\n#       \"\"\"Decodes a image based on the headers.\"\"\"\n#       return math_ops.cast(\n#           image_ops.decode_image(image_buffer, channels=self._channels),\n#           self._dtype)\n\n#     def decode_jpeg():\n#       \"\"\"Decodes a jpeg image with specified '_dct_method'.\"\"\"\n#       return math_ops.cast(\n#           image_ops.decode_jpeg(\n#               image_buffer,\n#               channels=self._channels,\n#               dct_method=self._dct_method), self._dtype)\n\n#     def check_jpeg():\n#       \"\"\"Checks if an image is jpeg.\"\"\"\n#       # For jpeg, we directly use image_ops.decode_jpeg rather than decode_image\n#       # in order to feed the jpeg specify parameter 'dct_method'.\n#       return tf.cond(\n#           image_ops.is_jpeg(image_buffer),\n#           decode_jpeg,\n#           decode_image,\n#           name='cond_jpeg')\n\n#     def decode_raw():\n#       \"\"\"Decodes a raw image.\"\"\"\n#       return parsing_ops.decode_raw(image_buffer, out_type=self._dtype)\n\n#     pred_fn_pairs = [(math_ops.logical_or(\n#         math_ops.equal(image_format, 'raw'),\n#         math_ops.equal(image_format, 'RAW')), decode_raw)]\n\n#     image = tf.case(\n#         pred_fn_pairs, default=check_jpeg, exclusive=True)\n\n#     image.set_shape([None, None, self._channels])\n#     if self._shape is not None:\n#       image = array_ops.reshape(image, self._shape)\n\n#     return image\n\n\n# class BoundingBoxSequence(ItemHandler):\n#   \"\"\"An ItemHandler that concatenates SparseTensors to Bounding Boxes.\n#   \"\"\"\n\n#   def __init__(self, keys=None, prefix=None, return_dense=True,\n#                default_value=-1.0):\n#     \"\"\"Initialize the bounding box handler.\n\n#     Args:\n#       keys: A list of four key names representing the ymin, xmin, ymax, xmax\n#         in the Example or SequenceExample.\n#       prefix: An optional prefix for each of the bounding box keys in the\n#         Example or SequenceExample. If provided, `prefix` is prepended to each\n#         key in `keys`.\n#       return_dense: if True, returns a dense tensor; if False, returns as\n#         sparse tensor.\n#       default_value: The value used when the `tensor_key` is not found in a\n#         particular `TFExample`.\n\n#     Raises:\n#       ValueError: if keys is not `None` and also not a list of exactly 4 keys\n#     \"\"\"\n#     if keys is None:\n#       keys = ['ymin', 'xmin', 'ymax', 'xmax']\n#     elif len(keys) != 4:\n#       raise ValueError('BoundingBoxSequence expects 4 keys but got {}'.format(\n#           len(keys)))\n#     self._prefix = prefix\n#     self._keys = keys\n#     self._full_keys = [prefix + k for k in keys]\n#     self._return_dense = return_dense\n#     self._default_value = default_value\n#     super(BoundingBoxSequence, self).__init__(self._full_keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n#     Args:\n#       keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n#     Returns:\n#       [time, num_boxes, 4] tensor of bounding box coordinates, in order\n#           [y_min, x_min, y_max, x_max]. Whether the tensor is a SparseTensor\n#           or a dense Tensor is determined by the return_dense parameter. Empty\n#           positions in the sparse tensor are filled with -1.0 values.\n#     \"\"\"\n#     sides = []\n#     for key in self._full_keys:\n#       value = keys_to_tensors[key]\n#       expanded_dims = array_ops.concat(\n#           [math_ops.to_int64(array_ops.shape(value)),\n#            constant_op.constant([1], dtype=dtypes.int64)], 0)\n#       side = sparse_ops.sparse_reshape(value, expanded_dims)\n#       sides.append(side)\n#     bounding_boxes = sparse_ops.sparse_concat(2, sides)\n#     if self._return_dense:\n#       bounding_boxes = sparse_ops.sparse_tensor_to_dense(\n#           bounding_boxes, default_value=self._default_value)\n#     return bounding_boxes\n\n\n# class NumBoxesSequence(ItemHandler):\n#   \"\"\"An ItemHandler that returns num_boxes per frame for a box sequence.\n\n#   `num_boxes` is inferred from a 2D SparseTensor decoded from a field in the\n#   SequenceExample. The SparseTensor is partially dense and only ragged along its\n#   second dimensions.\n\n#   The output is an int64 tf.Tensor of shape [time], which is solely determined\n#   by the tensor of the first key. However, if `check_consistency` is True, this\n#   function checks that `num_boxes` is consistent across all keys.\n#   \"\"\"\n\n#   def __init__(self, keys=None, check_consistency=True):\n#     \"\"\"Initialization.\n\n#     Args:\n#       keys: A list of keys of sparse tensors which have exactly 2 dimensions,\n#         with the 1st being the `time` and the 2nd the `boxes` per frame.\n#         key in `keys`.\n#       check_consistency: if True, check for consistency.\n\n#     Raises:\n#       ValueError: If keys is empty.\n#     \"\"\"\n#     if not keys:\n#       raise ValueError('keys must not be empty.')\n#     self._check_consistency = check_consistency\n#     super(NumBoxesSequence, self).__init__(keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"Maps the given dictionary of tensors to a num_boxes tensor.\n\n#     If check_consistency is True: raises runtime error in Tensorflow when the\n#     consistency is violated across tensors.\n\n#     Args:\n#       keys_to_tensors: A mapping of TF-Example keys to parsed tensors.\n\n#     Returns:\n#       [time] tf.Tensor containing the number of boxes per frame.\n\n#     Raises:\n#       ValueError: If any of the keyed tensors is not sparse or exactly 2\n#         dimensional.\n#     \"\"\"\n#     def _compute_num_boxes(tensor):\n#       \"\"\"Compute num_boxes from a single 2D tensor.\"\"\"\n#       if not isinstance(tensor, sparse_tensor.SparseTensor):\n#         raise ValueError('tensor must be of type tf.SparseTensor.')\n#       indices = tensor.indices\n#       dense_shape = tensor.dense_shape\n#       box_ids = indices[:, 1]\n#       box_ids = sparse_tensor.SparseTensor(\n#           indices=indices, values=box_ids, dense_shape=dense_shape)\n#       box_ids = sparse_ops.sparse_tensor_to_dense(box_ids, default_value=-1)\n#       # In the event that the parsed tensor is empty (perhaps due to a negative\n#       # example), we pad box_ids so that the resulting number of boxes is 0.\n#       num_boxes = math_ops.reduce_max(\n#           array_ops.pad(box_ids + 1, [[0, 0], [0, 1]]), axis=1)\n#       return num_boxes\n\n#     num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[0]])\n#     asserts = []\n#     if self._check_consistency:\n#       for i in range(1, len(self._keys)):\n#         cur_num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[i]])\n#         asserts.append(check_ops.assert_equal(num_boxes, cur_num_boxes))\n\n#     with ops.control_dependencies(asserts):\n#       return array_ops.identity(num_boxes)\n\n\n# class KeypointsSequence(ItemHandler):\n#   \"\"\"An ItemHandler that concatenates SparseTensors to Keypoints.\n#   \"\"\"\n\n#   def __init__(self, keys=None, prefix=None, return_dense=True,\n#                default_value=-1.0):\n#     \"\"\"Initialize the keypoints handler.\n\n#     Args:\n#       keys: A list of two key names representing the y and x coordinates in the\n#         Example or SequenceExample.\n#       prefix: An optional prefix for each of the keypoint keys in the Example\n#         or SequenceExample. If provided, `prefix` is prepended to each key in\n#         `keys`.\n#       return_dense: if True, returns a dense tensor; if False, returns as\n#         sparse tensor.\n#       default_value: The value used when the `tensor_key` is not found in a\n#         particular `TFExample`.\n\n#     Raises:\n#       ValueError: if keys is not `None` and also not a list of exactly 2 keys\n#     \"\"\"\n#     if keys is None:\n#       keys = ['y', 'x']\n#     elif len(keys) != 2:\n#       raise ValueError('KeypointsSequence expects 2 keys but got {}'.format(\n#           len(keys)))\n#     self._prefix = prefix\n#     self._keys = keys\n#     self._full_keys = [prefix + k for k in keys]\n#     self._return_dense = return_dense\n#     self._default_value = default_value\n#     super(KeypointsSequence, self).__init__(self._full_keys)\n\n#   def tensors_to_item(self, keys_to_tensors):\n#     \"\"\"Maps the given dictionary of tensors to a concatenated list of keypoints.\n\n#     Args:\n#       keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n#     Returns:\n#       [time, num_keypoints, 2] tensor of keypoint coordinates, in order [y, x].\n#           Whether the tensor is a SparseTensor or a dense Tensor is determined\n#           by the return_dense parameter. Empty positions in the sparse tensor\n#           are filled with -1.0 values.\n#     \"\"\"\n#     coordinates = []\n#     for key in self._full_keys:\n#       value = keys_to_tensors[key]\n#       expanded_dims = array_ops.concat(\n#           [math_ops.to_int64(array_ops.shape(value)),\n#            constant_op.constant([1], dtype=dtypes.int64)], 0)\n#       coordinate = sparse_ops.sparse_reshape(value, expanded_dims)\n#       coordinates.append(coordinate)\n#     keypoints = sparse_ops.sparse_concat(2, coordinates)\n#     if self._return_dense:\n#       keypoints = sparse_ops.sparse_tensor_to_dense(\n#           keypoints, default_value=self._default_value)\n#     return keypoints\n\n\n# class TFExampleDecoder(data_decoder.DataDecoder):\n#   \"\"\"A decoder for TensorFlow Examples.\n\n#   Decoding Example proto buffers is comprised of two stages: (1) Example parsing\n#   and (2) tensor manipulation.\n\n#   In the first stage, the tf.io.parse_example function is called with a list of\n#   FixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\n#   the example. The output of this stage is a set of tensors.\n\n#   In the second stage, the resulting tensors are manipulated to provide the\n#   requested 'item' tensors.\n\n#   To perform this decoding operation, an ExampleDecoder is given a list of\n#   ItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\n#   contains the instructions for post_processing its tensors for stage 2.\n#   \"\"\"\n\n#   def __init__(self, keys_to_features, items_to_handlers):\n#     \"\"\"Constructs the decoder.\n\n#     Args:\n#       keys_to_features: a dictionary from TF-Example keys to either\n#         tf.io.VarLenFeature or tf.io.FixedLenFeature instances. See tensorflow's\n#         parsing_ops.py.\n#       items_to_handlers: a dictionary from items (strings) to ItemHandler\n#         instances. Note that the ItemHandler's are provided the keys that they\n#         use to return the final item Tensors.\n#     \"\"\"\n#     self._keys_to_features = keys_to_features\n#     self._items_to_handlers = items_to_handlers\n\n#   def list_items(self):\n#     \"\"\"See base class.\"\"\"\n#     return list(self._items_to_handlers.keys())\n\n#   def decode(self, serialized_example, items=None):\n#     \"\"\"Decodes the given serialized TF-example.\n\n#     Args:\n#       serialized_example: a serialized TF-example tensor.\n#       items: the list of items to decode. These must be a subset of the item\n#         keys in self._items_to_handlers. If `items` is left as None, then all\n#         of the items in self._items_to_handlers are decoded.\n\n#     Returns:\n#       the decoded items, a list of tensor.\n#     \"\"\"\n#     example = parsing_ops.parse_single_example(serialized_example,\n#                                                self._keys_to_features)\n\n#     # Reshape non-sparse elements just once, adding the reshape ops in\n#     # deterministic order.\n#     for k in sorted(self._keys_to_features):\n#       v = self._keys_to_features[k]\n#       if isinstance(v, parsing_ops.FixedLenFeature):\n#         example[k] = array_ops.reshape(example[k], v.shape)\n\n#     if not items:\n#       items = self._items_to_handlers.keys()\n\n#     outputs = []\n#     for item in items:\n#       handler = self._items_to_handlers[item]\n#       keys_to_tensors = {key: example[key] for key in handler.keys}\n#       outputs.append(handler.tensors_to_item(keys_to_tensors))\n#     return outputs\n\n\n# class TFSequenceExampleDecoder(data_decoder.DataDecoder):\n#   \"\"\"A decoder for TensorFlow SequenceExamples.\n\n#   Decoding SequenceExample proto buffers is comprised of two stages:\n#   (1) Example parsing and (2) tensor manipulation.\n\n#   In the first stage, the tf.parse_single_sequence_example function is called\n#   with a list of FixedLenFeatures and SparseLenFeatures. These instances tell TF\n#   how to parse the example. The output of this stage is a set of tensors.\n\n#   In the second stage, the resulting tensors are manipulated to provide the\n#   requested 'item' tensors.\n\n#   To perform this decoding operation, a SequenceExampleDecoder is given a list\n#   of ItemHandlers. Each ItemHandler indicates the set of features for stage 1\n#   and contains the instructions for post_processing its tensors for stage 2.\n#   \"\"\"\n\n#   def __init__(self, keys_to_context_features, keys_to_sequence_features,\n#                items_to_handlers):\n#     \"\"\"Constructs the decoder.\n\n#     Args:\n#       keys_to_context_features: a dictionary from TF-SequenceExample context\n#         keys to either tf.VarLenFeature or tf.FixedLenFeature instances.\n#         See tensorflow's parsing_ops.py.\n#       keys_to_sequence_features: a dictionary from TF-SequenceExample sequence\n#         keys to either tf.VarLenFeature or tf.FixedLenSequenceFeature instances.\n#         See tensorflow's parsing_ops.py.\n#       items_to_handlers: a dictionary from items (strings) to ItemHandler\n#         instances. Note that the ItemHandler's are provided the keys that they\n#         use to return the final item Tensors.\n\n#     Raises:\n#       ValueError: if the same key is present for context features and sequence\n#         features.\n#     \"\"\"\n#     unique_keys = set()\n#     unique_keys.update(keys_to_context_features)\n#     unique_keys.update(keys_to_sequence_features)\n\n#     self._keys_to_context_features = keys_to_context_features\n#     self._keys_to_sequence_features = keys_to_sequence_features\n#     self._items_to_handlers = items_to_handlers\n\n#   def list_items(self):\n#     \"\"\"See base class.\"\"\"\n#     return self._items_to_handlers.keys()\n\n#   def decode(self, serialized_example, items=None):\n#     \"\"\"Decodes the given serialized TF-SequenceExample.\n\n#     Args:\n#       serialized_example: a serialized TF-SequenceExample tensor.\n#       items: the list of items to decode. These must be a subset of the item\n#         keys in self._items_to_handlers. If `items` is left as None, then all\n#         of the items in self._items_to_handlers are decoded.\n\n#     Returns:\n#       the decoded items, a list of tensor.\n#     \"\"\"\n\n#     context, feature_list = parsing_ops.parse_single_sequence_example(\n#         serialized_example, self._keys_to_context_features,\n#         self._keys_to_sequence_features)\n\n#     # Reshape non-sparse elements just once:\n#     for k in self._keys_to_context_features:\n#       v = self._keys_to_context_features[k]\n#       if isinstance(v, parsing_ops.FixedLenFeature):\n#         context[k] = array_ops.reshape(context[k], v.shape)\n\n#     if not items:\n#       items = self._items_to_handlers.keys()\n\n#     outputs = []\n#     for item in items:\n#       handler = self._items_to_handlers[item]\n#       keys_to_tensors = {\n#           key: context[key] if key in context else feature_list[key]\n#           for key in handler.keys\n#       }\n#       outputs.append(handler.tensors_to_item(keys_to_tensors))\n#     return outputs'''\n\n# with open('/usr/local/lib/python3.10/site-packages/tf_slim/data/tfexample_decoder.py', 'w') as f:\n#     f.write(file)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:54.110443Z","iopub.execute_input":"2024-07-27T13:33:54.110729Z","iopub.status.idle":"2024-07-27T13:33:54.144234Z","shell.execute_reply.started":"2024-07-27T13:33:54.110705Z","shell.execute_reply":"2024-07-27T13:33:54.143419Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport struct\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom object_detection.utils import config_util\nfrom object_detection.protos import pipeline_pb2, optimizer_pb2\n\nfrom google.protobuf import text_format\n\n%matplotlib inline\n\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:54.145062Z","iopub.execute_input":"2024-07-27T13:33:54.145293Z","iopub.status.idle":"2024-07-27T13:33:59.453105Z","shell.execute_reply.started":"2024-07-27T13:33:54.145272Z","shell.execute_reply":"2024-07-27T13:33:59.452238Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-07-27 13:33:54.550983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-27 13:33:54.551043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-27 13:33:54.552522: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Helper Functions:","metadata":{}},{"cell_type":"code","source":"def adjust_config_file(config_file, fine_tune_checkpoint, model_name, lr_type='manual_step', resume=False):\n    # Read the configuration file\n    configs = pipeline_pb2.TrainEvalPipelineConfig()\n    with tf.io.gfile.GFile(config_file, \"r\") as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, configs)\n    \n    # Adjust the configuration file\n    if model_name == 'faster_rcnn':\n        configs.model.faster_rcnn.num_classes = num_classes\n    elif model_name == 'ssd':\n        configs.model.ssd.num_classes = num_classes\n    \n    configs.train_config.batch_size = batch_size\n    configs.train_config.fine_tune_checkpoint_type = fine_tune_checkpoint_type\n    configs.train_config.fine_tune_checkpoint = fine_tune_checkpoint\n    configs.train_config.num_steps = total_steps\n    configs.train_config.use_bfloat16 = False\n    \n    # Freeze the layers\n    configs.train_config.freeze_variables.append(\"FeatureExtractor/*\")\n\n    # Cosine Decay Learning Rate\n    if lr_type == 'cosine_decay':\n        configs.train_config.optimizer.adam_optimizer.learning_rate.cosine_decay_learning_rate.total_steps = total_steps\n        configs.train_config.optimizer.adam_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = 0.6 * learning_rate\n        configs.train_config.optimizer.adam_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = learning_rate\n        configs.train_config.optimizer.adam_optimizer.learning_rate.cosine_decay_learning_rate.warmup_steps = steps_per_epoch\n    \n    # Manual Step Learning Rate\n    elif lr_type == 'manual_step':\n        learning_rate_schedule = optimizer_pb2.ManualStepLearningRate(\n            initial_learning_rate=learning_rate,\n            warmup=True,\n            schedule=[\n                optimizer_pb2.ManualStepLearningRate.LearningRateSchedule(\n                    step=int(0.7*total_steps),\n                    learning_rate=learning_rate/10\n                ),\n            ]\n        )\n        configs.train_config.optimizer.momentum_optimizer.learning_rate.manual_step_learning_rate.CopyFrom(learning_rate_schedule)\n\n    # Constant Learning Rate\n    elif lr_type == 'constant':\n        configs.train_config.optimizer.adam_optimizer.learning_rate.constant_learning_rate.learning_rate = learning_rate/10\n    \n    # To resume from last chackpoint\n    if resume:\n        configs.train_config.fine_tune_checkpoint_type = 'full'\n    \n    configs.train_input_reader.label_map_path = label_path\n    configs.train_input_reader.tf_record_input_reader.input_path[0] = train_path\n    \n    configs.eval_input_reader[0].label_map_path = label_path\n    configs.eval_input_reader[0].tf_record_input_reader.input_path[0] = valid_path\n    \n    # Save the configuration file\n    config_text = text_format.MessageToString(configs)\n    with tf.io.gfile.GFile(config_file, \"w\") as f:\n        f.write(config_text) ","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:59.454639Z","iopub.execute_input":"2024-07-27T13:33:59.455131Z","iopub.status.idle":"2024-07-27T13:33:59.467883Z","shell.execute_reply.started":"2024-07-27T13:33:59.455102Z","shell.execute_reply":"2024-07-27T13:33:59.466764Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(checkpoint_dir, model_config, num_steps, record_summaries=True, eval_on_train_data=True):\n    # Train the model\n    !python {model_dir} --model_dir={checkpoint_dir} --pipeline_config_path={model_config} --num_train_steps={num_steps} --checkpoint_every_n={steps_per_epoch} --record_summaries={record_summaries} --eval_on_train_data={eval_on_train_data} --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:59.469342Z","iopub.execute_input":"2024-07-27T13:33:59.469806Z","iopub.status.idle":"2024-07-27T13:33:59.482184Z","shell.execute_reply.started":"2024-07-27T13:33:59.469774Z","shell.execute_reply":"2024-07-27T13:33:59.481381Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# function to plot the training loss\ndef plot_loss(log_dir, tag):\n    log_dir += '/train'\n    loss_values = []\n    \n    for file_name in os.listdir(log_dir):\n        if file_name.startswith('events'):\n            log_file = os.path.join(log_dir, file_name)\n            for event in tf.compat.v1.train.summary_iterator(log_file):\n                for value in event.summary.value:\n                    if value.tag == tag:\n                        tensor_content = value.tensor.tensor_content\n                        loss_value = struct.unpack('f', tensor_content)\n                        loss_values.append(loss_value[0])\n    \n    if tag == 'learning_rate':\n        plt.plot([x*100 for x in range(len(loss_values))], loss_values)\n        plt.xlabel(\"Steps\")\n        plt.ylabel(\"Learning Rate\")\n        plt.title(\"Training Learning Rate\")\n        plt.show()\n        return\n        \n    plt.plot([x*100 for x in range(len(loss_values))], loss_values)\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:59.483372Z","iopub.execute_input":"2024-07-27T13:33:59.483864Z","iopub.status.idle":"2024-07-27T13:33:59.493986Z","shell.execute_reply.started":"2024-07-27T13:33:59.483833Z","shell.execute_reply":"2024-07-27T13:33:59.493216Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Downloading the model","metadata":{}},{"cell_type":"code","source":"# AUTO = tf.data.experimental.AUTOTUNE\n# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:59.495046Z","iopub.execute_input":"2024-07-27T13:33:59.495703Z","iopub.status.idle":"2024-07-27T13:33:59.505900Z","shell.execute_reply.started":"2024-07-27T13:33:59.495672Z","shell.execute_reply":"2024-07-27T13:33:59.505079Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model = 'faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz'\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/{model}\n!tar -xvzf {model}\n!rm {model}","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:33:59.506910Z","iopub.execute_input":"2024-07-27T13:33:59.507349Z","iopub.status.idle":"2024-07-27T13:34:19.098559Z","shell.execute_reply.started":"2024-07-27T13:33:59.507325Z","shell.execute_reply":"2024-07-27T13:34:19.097257Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-07-27 13:34:00--  http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\nResolving download.tensorflow.org (download.tensorflow.org)... 142.251.18.207, 172.217.218.207, 142.250.153.207, ...\nConnecting to download.tensorflow.org (download.tensorflow.org)|142.251.18.207|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 470656289 (449M) [application/x-tar]\nSaving to: 'faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz'\n\nfaster_rcnn_resnet1 100%[===================>] 448.85M  41.6MB/s    in 11s     \n\n2024-07-27 13:34:12 (39.1 MB/s) - 'faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz' saved [470656289/470656289]\n\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/checkpoint\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/pipeline.config\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/saved_model/\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/saved_model/saved_model.pb\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/saved_model/variables/\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\nfaster_rcnn_resnet152_v1_640x640_coco17_tpu-8/saved_model/variables/variables.index\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!mkdir configs\n!cp /kaggle/input/tf-od-configs/configs/Faster_RCNN/RCNN_Resnet152/pipeline.config /kaggle/working/configs/","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:19.100314Z","iopub.execute_input":"2024-07-27T13:34:19.100722Z","iopub.status.idle":"2024-07-27T13:34:21.145669Z","shell.execute_reply.started":"2024-07-27T13:34:19.100686Z","shell.execute_reply":"2024-07-27T13:34:21.144361Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"output_path = 'output'\n!mkdir {output_path}\n\nmodel_path = '/kaggle/working/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8'\nconfig_path = '/kaggle/working/configs/pipeline.config'\n\n# To start from scratch\nfine_tune_path = model_path + '/checkpoint/ckpt-0'\n\n# # To resume from last checkpoint, replace ?? with the appropriate number\n# !mkdir checkpoints\n# !cp -r /kaggle/input/checkpoints/ckpts/. /kaggle/working/checkpoints\n# fine_tune_path = '/kaggle/working/checkpoints/ckpt-35'\n\nlabel_path = '/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle_label_map.pbtxt'\ntrain_path = '/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle.tfrecord'\nvalid_path = '/kaggle/input/traffic-od-tfrecords/annotations/valid/car-truck-motorcycle.tfrecord'\n\nmodel_dir = 'models/research/object_detection/model_main_tf2.py'\n\nwith open(config_path, 'r') as f:\n    print(f.read())","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:21.147246Z","iopub.execute_input":"2024-07-27T13:34:21.147576Z","iopub.status.idle":"2024-07-27T13:34:22.152080Z","shell.execute_reply.started":"2024-07-27T13:34:21.147545Z","shell.execute_reply":"2024-07-27T13:34:22.150909Z"},"trusted":true},"outputs":[{"name":"stdout","text":"model {\n  faster_rcnn {\n    num_classes: 6\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 640\n        max_dimension: 640\n        pad_to_max_dimension: true\n      }\n    }\n    feature_extractor {\n      type: \"faster_rcnn_resnet152_keras\"\n      batch_norm_trainable: true\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        height_stride: 16\n        width_stride: 16\n        scales: 0.25\n        scales: 0.5\n        scales: 1.0\n        scales: 2.0\n        aspect_ratios: 0.5\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        share_box_across_classes: true\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n    use_matmul_crop_and_resize: true\n    clip_anchors_to_image: true\n    use_matmul_gather_in_matcher: true\n    use_static_balanced_label_sampler: true\n    use_static_shapes: true\n  }\n}\ntrain_config {\n  batch_size: 2\n  sync_replicas: true\n  optimizer {\n    adam_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.001\n          total_steps: 22698\n          warmup_learning_rate: 0.0006\n          warmup_steps: 3783\n        }\n      }\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"/mnt/d/Scripts/pretrained_models/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n  num_steps: 22698\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\"\n  use_bfloat16: false\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: \"/mnt/d/MastersProject/data/images/annotations/train/person-car-bicycle-bus-motorbike_label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"/mnt/d/MastersProject/data/images/annotations/train/person-car-bicycle-bus-motorbike.tfrecord\"\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1\n}\neval_input_reader {\n  label_map_path: \"/mnt/d/MastersProject/data/images/annotations/train/person-car-bicycle-bus-motorbike_label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"/mnt/d/MastersProject/data/images/annotations/valid/person-car-bicycle-bus-motorbike.tfrecord\"\n  }\n}\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# os.listdir('checkpoints')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.153698Z","iopub.execute_input":"2024-07-27T13:34:22.154010Z","iopub.status.idle":"2024-07-27T13:34:22.158566Z","shell.execute_reply.started":"2024-07-27T13:34:22.153982Z","shell.execute_reply":"2024-07-27T13:34:22.157459Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"num_classes = 5\nfine_tune_checkpoint_type = 'detection'\n\nbatch_size = 4\nlearning_rate = 0.001\n\nsteps_per_epoch = math.ceil(1999 / batch_size)\nepochs = 20\ntotal_steps = steps_per_epoch * epochs","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.159658Z","iopub.execute_input":"2024-07-27T13:34:22.159934Z","iopub.status.idle":"2024-07-27T13:34:22.168634Z","shell.execute_reply.started":"2024-07-27T13:34:22.159911Z","shell.execute_reply":"2024-07-27T13:34:22.167787Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"total_steps","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.169741Z","iopub.execute_input":"2024-07-27T13:34:22.169983Z","iopub.status.idle":"2024-07-27T13:34:22.180342Z","shell.execute_reply.started":"2024-07-27T13:34:22.169962Z","shell.execute_reply":"2024-07-27T13:34:22.179334Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"10000"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"model_name = 'faster_rcnn'\nadjust_config_file(config_path, fine_tune_path, model_name, lr_type='manual_step', resume=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.181557Z","iopub.execute_input":"2024-07-27T13:34:22.181878Z","iopub.status.idle":"2024-07-27T13:34:22.195328Z","shell.execute_reply.started":"2024-07-27T13:34:22.181849Z","shell.execute_reply":"2024-07-27T13:34:22.194305Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"with open(config_path, 'r') as f:\n    print(f.read())","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.196601Z","iopub.execute_input":"2024-07-27T13:34:22.197280Z","iopub.status.idle":"2024-07-27T13:34:22.202130Z","shell.execute_reply.started":"2024-07-27T13:34:22.197249Z","shell.execute_reply":"2024-07-27T13:34:22.201305Z"},"trusted":true},"outputs":[{"name":"stdout","text":"model {\n  faster_rcnn {\n    num_classes: 5\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 640\n        max_dimension: 640\n        pad_to_max_dimension: true\n      }\n    }\n    feature_extractor {\n      type: \"faster_rcnn_resnet152_keras\"\n      batch_norm_trainable: true\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        height_stride: 16\n        width_stride: 16\n        scales: 0.25\n        scales: 0.5\n        scales: 1.0\n        scales: 2.0\n        aspect_ratios: 0.5\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        share_box_across_classes: true\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n    use_matmul_crop_and_resize: true\n    clip_anchors_to_image: true\n    use_matmul_gather_in_matcher: true\n    use_static_balanced_label_sampler: true\n    use_static_shapes: true\n  }\n}\ntrain_config {\n  batch_size: 4\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        manual_step_learning_rate {\n          initial_learning_rate: 0.001\n          schedule {\n            step: 7000\n            learning_rate: 0.0001\n          }\n          warmup: true\n        }\n      }\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"/kaggle/working/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n  num_steps: 10000\n  startup_delay_steps: 0.0\n  freeze_variables: \"FeatureExtractor/*\"\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\"\n  use_bfloat16: false\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: \"/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle_label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle.tfrecord\"\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1\n}\neval_input_reader {\n  label_map_path: \"/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle_label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"/kaggle/input/traffic-od-tfrecords/annotations/valid/car-truck-motorcycle.tfrecord\"\n  }\n}\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"train_model(output_path, config_path, total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:34:22.203352Z","iopub.execute_input":"2024-07-27T13:34:22.203777Z","iopub.status.idle":"2024-07-27T13:38:02.951170Z","shell.execute_reply.started":"2024-07-27T13:34:22.203746Z","shell.execute_reply":"2024-07-27T13:38:02.950194Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2024-07-27 13:34:23.692809: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-27 13:34:23.692869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-27 13:34:23.694169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0727 13:34:28.816703 132234833557312 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\nI0727 13:34:29.137356 132234833557312 config_util.py:552] Maybe overwriting train_steps: 10000\nI0727 13:34:29.137686 132234833557312 config_util.py:552] Maybe overwriting use_bfloat16: False\nW0727 13:34:29.182345 132234833557312 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\nInstructions for updating:\nrename to distribute_datasets_from_function\nI0727 13:34:29.200417 132234833557312 dataset_builder.py:162] Reading unweighted datasets: ['/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle.tfrecord']\nI0727 13:34:29.202595 132234833557312 dataset_builder.py:79] Reading record datasets for input file: ['/kaggle/input/traffic-od-tfrecords/annotations/train/car-truck-motorcycle.tfrecord']\nI0727 13:34:29.202737 132234833557312 dataset_builder.py:80] Number of filenames to read: 1\nW0727 13:34:29.202862 132234833557312 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\nW0727 13:34:29.209975 132234833557312 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\nW0727 13:34:29.230924 132234833557312 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map()\nW0727 13:34:34.493730 132234833557312 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\nW0727 13:34:37.742573 132234833557312 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\n/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n  warnings.warn(\nI0727 13:34:53.977062 132229149746944 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\nW0727 13:35:05.495327 132229149746944 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.tensor) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse ref() instead.\nW0727 13:35:12.559038 132229149746944 deprecation.py:50] From /opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nI0727 13:35:36.675438 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.678247 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.679403 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.680400 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.684260 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.685229 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.686313 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.687272 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.690873 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:35:36.691932 132234833557312 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nI0727 13:37:58.942583 132234833557312 model_lib_v2.py:705] Step 100 per-step time 1.403s\nI0727 13:37:58.943046 132234833557312 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.6555172,\n 'Loss/BoxClassifierLoss/localization_loss': 0.73241675,\n 'Loss/RPNLoss/localization_loss': 0.19591412,\n 'Loss/RPNLoss/objectness_loss': 0.22326955,\n 'Loss/regularization_loss': 0.0,\n 'Loss/total_loss': 1.8071177,\n 'learning_rate': 0.0009871429}\n^C\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"plot_loss(output_path, 'Loss/total_loss')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:02.952630Z","iopub.execute_input":"2024-07-27T13:38:02.952945Z","iopub.status.idle":"2024-07-27T13:38:03.210189Z","shell.execute_reply.started":"2024-07-27T13:38:02.952916Z","shell.execute_reply":"2024-07-27T13:38:03.209215Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"plot_loss(output_path, 'learning_rate')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:03.211503Z","iopub.execute_input":"2024-07-27T13:38:03.212135Z","iopub.status.idle":"2024-07-27T13:38:03.383821Z","shell.execute_reply.started":"2024-07-27T13:38:03.212100Z","shell.execute_reply":"2024-07-27T13:38:03.382870Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import shutil\nOUTPUT_NAME = '/kaggle/working/checkpoints'\nDIRECTORY_TO_ZIP = '/kaggle/working/output'\n\nshutil.make_archive(OUTPUT_NAME, 'zip', DIRECTORY_TO_ZIP)\n\n# Wait until the cell finishes then download the file by hovering on it\n# If the folder is too large, then just download the last checpoints in output folder, both ckpt-(max number).* \n# then after downloading and making sure of the size, restart the session and alter the parts needed","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:03.384934Z","iopub.execute_input":"2024-07-27T13:38:03.385208Z","iopub.status.idle":"2024-07-27T13:38:04.317893Z","shell.execute_reply.started":"2024-07-27T13:38:03.385184Z","shell.execute_reply":"2024-07-27T13:38:04.316002Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"tf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:04.318829Z","iopub.status.idle":"2024-07-27T13:38:04.319159Z","shell.execute_reply.started":"2024-07-27T13:38:04.318998Z","shell.execute_reply":"2024-07-27T13:38:04.319011Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:04.320475Z","iopub.status.idle":"2024-07-27T13:38:04.320820Z","shell.execute_reply.started":"2024-07-27T13:38:04.320658Z","shell.execute_reply":"2024-07-27T13:38:04.320671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi -r","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:04.321755Z","iopub.status.idle":"2024-07-27T13:38:04.322177Z","shell.execute_reply.started":"2024-07-27T13:38:04.321955Z","shell.execute_reply":"2024-07-27T13:38:04.321974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntf.keras.backend.clear_session()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:04.323284Z","iopub.status.idle":"2024-07-27T13:38:04.323698Z","shell.execute_reply.started":"2024-07-27T13:38:04.323520Z","shell.execute_reply":"2024-07-27T13:38:04.323539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import backend as K\nimport gc\n# After each iteration:\nK.clear_session()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:38:04.324629Z","iopub.status.idle":"2024-07-27T13:38:04.325113Z","shell.execute_reply.started":"2024-07-27T13:38:04.324884Z","shell.execute_reply":"2024-07-27T13:38:04.324904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}